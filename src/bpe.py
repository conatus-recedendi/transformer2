"""
ì‹¤ì œ WMT ë°ì´í„° ë¡œë”© ëª¨ë“ˆ
data/wmt14_en_de/train.txt, valid.txt, test.txt í˜•ì‹ìœ¼ë¡œ ì €ì¥ëœ ë°ì´í„° ë¡œë“œ
"""

import os
import torch
from pathlib import Path
from typing import List, Tuple, Dict, Optional
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import logging
import sentencepiece as spm

logger = logging.getLogger(__name__)


class BPEVocabulary:
    """BPE(Byte Pair Encoding) ê¸°ë°˜ ì–´íœ˜ ì‚¬ì „"""

    def __init__(self):
        self.sp_model = None
        self.vocab_size = 30000
        self.special_tokens = {"<PAD>": 0, "<BOS>": 1, "<EOS>": 2, "<UNK>": 3}
        self.model_path = None

    def train_bpe_model(
        self,
        file_paths: List[str],
        vocab_size: int = 30000,
        model_prefix: str = "bpe_model",
    ):
        """BPE ëª¨ë¸ í›ˆë ¨"""
        logger.info(f"Training BPE model from {len(file_paths)} files...")

        self.vocab_size = vocab_size
        self.model_path = f"{model_prefix}.model"

        # ëª¨ë“  íŒŒì¼ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        combined_file = f"{model_prefix}_combined.txt"
        total_lines = 0

        with open(combined_file, "w", encoding="utf-8") as outf:
            for file_path in file_paths:
                if os.path.exists(file_path):
                    logger.info(f"Processing {file_path}...")
                    with open(file_path, "r", encoding="utf-8") as inf:
                        for line in inf:
                            line = line.strip()
                            if line:
                                outf.write(line + "\n")
                                total_lines += 1

                            if total_lines % 1_000_000 == 0:
                                logger.info(f"  Processed {total_lines} lines...")

        logger.info(f"Total lines for BPE training: {total_lines}")

        # BPE ëª¨ë¸ í›ˆë ¨
        spm.SentencePieceTrainer.train(
            input=combined_file,
            model_prefix=model_prefix,
            vocab_size=vocab_size,
            character_coverage=0.995,
            model_type="bpe",
            # íŠ¹ìˆ˜ í† í° IDë¥¼ ëª…í™•íˆ ë¶„ë¦¬í•˜ì—¬ ì„¤ì •
            pad_id=0,
            bos_id=1,
            eos_id=2,
            unk_id=3,
            # íŠ¹ìˆ˜ í† í° ë¬¸ìì—´ ì„¤ì •
            pad_piece="<PAD>",
            bos_piece="<BOS>",
            eos_piece="<EOS>",
            unk_piece="<UNK>",
            # user_defined_symbolsì—ì„œ UNK ì œì™¸ (ìë™ìœ¼ë¡œ ì •ì˜ë¨)
            user_defined_symbols=[],
            # ì¶”ê°€ ì„¤ì •ìœ¼ë¡œ ì •í™•í•œ ID ë§¤í•‘ ë³´ì¥
            control_symbols=["<PAD>", "<BOS>", "<EOS>"],
        )

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(combined_file)

        # ëª¨ë¸ ë¡œë“œ
        self.load_model(self.model_path)

        logger.info(f"BPE model trained and saved: {self.model_path}")
        logger.info(f"Vocabulary size: {len(self)}")

        # í›ˆë ¨ ì§í›„ íŠ¹ìˆ˜ í† í° ê²€ì¦
        logger.info("Verifying special tokens after training:")
        for token, expected_id in self.special_tokens.items():
            actual_id = self.sp_model.piece_to_id(token)
            if actual_id == expected_id:
                logger.info(f"  âœ“ {token}: {actual_id}")
            else:
                logger.error(f"  âœ— {token}: expected {expected_id}, got {actual_id}")
                raise ValueError(
                    f"BPE training failed: {token} has wrong ID {actual_id}, expected {expected_id}"
                )

    def load_model(self, model_path: str):
        """í›ˆë ¨ëœ BPE ëª¨ë¸ ë¡œë“œ"""
        self.sp_model = spm.SentencePieceProcessor()
        self.sp_model.load(model_path)
        self.model_path = model_path

        # íŠ¹ìˆ˜ í† í° ID ê²€ì¦ ë° ìˆ˜ì •
        self._verify_special_tokens()

        logger.info(f"BPE model loaded from {model_path}")
        logger.info(f"Vocabulary size: {len(self)}")
        logger.info(f"Special token mapping:")
        for token, expected_id in self.special_tokens.items():
            actual_id = self.sp_model.piece_to_id(token)
            logger.info(f"  {token}: expected={expected_id}, actual={actual_id}")

    def _verify_special_tokens(self):
        """íŠ¹ìˆ˜ í† í° IDê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ ê²€ì¦"""
        for token, expected_id in self.special_tokens.items():
            actual_id = self.sp_model.piece_to_id(token)
            if actual_id != expected_id:
                logger.warning(
                    f"Special token ID mismatch: {token} expected={expected_id}, actual={actual_id}"
                )

        # ì–´íœ˜ í¬ê¸° í™•ì¸
        vocab_size = self.sp_model.get_piece_size()
        logger.info(f"Loaded vocabulary size: {vocab_size}")

        # ì²˜ìŒ ëª‡ ê°œ í† í° í™•ì¸
        logger.info("First 10 tokens:")
        for i in range(min(10, vocab_size)):
            piece = self.sp_model.id_to_piece(i)
            logger.info(f"  ID {i}: '{piece}'")

        # íŠ¹ìˆ˜ í† í°ì´ ì œëŒ€ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸
        for token in ["<PAD>", "<BOS>", "<EOS>", "<UNK>"]:
            token_id = self.sp_model.piece_to_id(token)
            if token_id < 0:
                logger.error(f"Invalid token ID for {token}: {token_id}")
            else:
                logger.info(f"Valid token: {token} -> ID {token_id}")

    def encode(self, text: str) -> List[int]:
        """í…ìŠ¤íŠ¸ë¥¼ BPE í† í° IDë¡œ ë³€í™˜"""
        if self.sp_model is None:
            raise ValueError(
                "BPE model not loaded. Call train_bpe_model() or load_model() first."
            )

        if isinstance(text, list):
            # í† í° ë¦¬ìŠ¤íŠ¸ê°€ ì…ë ¥ëœ ê²½ìš° ê³µë°±ìœ¼ë¡œ ê²°í•©
            text = " ".join(text)

        return self.sp_model.encode_as_ids(text)

    def decode(self, ids: List[int]) -> str:
        """BPE í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if self.sp_model is None:
            raise ValueError("BPE model not loaded.")

        return self.sp_model.decode_ids(ids)

    def encode_as_pieces(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ BPE í† í° ì¡°ê°ìœ¼ë¡œ ë³€í™˜"""
        if self.sp_model is None:
            raise ValueError("BPE model not loaded.")

        if isinstance(text, list):
            text = " ".join(text)

        return self.sp_model.encode_as_pieces(text)

    def __len__(self):
        if self.sp_model is None:
            return self.vocab_size
        return self.sp_model.get_piece_size()


class RealWMTDataset(Dataset):
    """ì‹¤ì œ WMT ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ë¶„ë¦¬ëœ ì–¸ì–´ íŒŒì¼ í˜•ì‹: train.en, train.de)"""

    def __init__(
        self,
        src_file: str,
        tgt_file: str,
        vocab: BPEVocabulary,
        max_length: int = 512,
        apply_cleaning: bool = True,
    ):
        self.src_file = src_file
        self.tgt_file = tgt_file
        self.vocab = vocab
        self.max_length = max_length
        self.apply_cleaning = apply_cleaning

        # ë°ì´í„° ë¡œë“œ
        self.data_pairs = self._load_data()

        logger.info(f"Loaded {len(self.data_pairs)} sentence pairs")
        logger.info(f"  Source file: {src_file}")
        logger.info(f"  Target file: {tgt_file}")
        logger.info(f"  Data cleaning: {'Enabled' if apply_cleaning else 'Disabled'}")

    def _load_data(self) -> List[Tuple[str, str]]:
        """ë¶„ë¦¬ëœ ì–¸ì–´ íŒŒì¼ë“¤ ë¡œë“œ (BPEìš©ìœ¼ë¡œ ì›ë¬¸ í…ìŠ¤íŠ¸ ë°˜í™˜)"""
        data_pairs = []

        if not os.path.exists(self.src_file) or not os.path.exists(self.tgt_file):
            logger.warning(f"Data files not found: {self.src_file} or {self.tgt_file}")
            return data_pairs

        # ğŸ” ë°”ì´ë„ˆë¦¬ ëª¨ë“œë¡œ ì •í™•í•œ ë¼ì¸ ìˆ˜ í™•ì¸
        def count_binary_lines(file_path):
            with open(file_path, "rb") as f:
                return f.read().count(b"\n")

        src_line_count = count_binary_lines(self.src_file)
        tgt_line_count = count_binary_lines(self.tgt_file)

        if src_line_count != tgt_line_count:
            logger.error(f"âŒ Binary line count mismatch:")
            logger.error(f"  {self.src_file}: {src_line_count:,} lines")
            logger.error(f"  {self.tgt_file}: {tgt_line_count:,} lines")
            raise ValueError(
                "Source and target files must have the same number of lines"
            )

        logger.info(f"ğŸ“Š Binary line counts match: {src_line_count:,} lines each")

        # ğŸš¨ ë‹¨ë… \r ë¬¸ì ë¬¸ì œ ê°ì§€ ë° ê²½ê³ 
        def check_standalone_cr(file_path):
            with open(file_path, "rb") as f:
                data = f.read()
                cr_count = data.count(b"\r")
                crlf_count = data.count(b"\r\n")
                standalone_cr = cr_count - crlf_count
                return standalone_cr

        src_standalone_cr = check_standalone_cr(self.src_file)
        tgt_standalone_cr = check_standalone_cr(self.tgt_file)

        if src_standalone_cr > 0 or tgt_standalone_cr > 0:
            logger.warning(f"âš ï¸  Standalone \\r characters detected:")
            logger.warning(f"  {self.src_file}: {src_standalone_cr} standalone \\r")
            logger.warning(f"  {self.tgt_file}: {tgt_standalone_cr} standalone \\r")
            logger.warning(f"  Using newlines='\\n' mode to prevent misalignment")

        # ğŸš¨ ê°•ë ¥í•œ íŒŒì¼ ì½ê¸° - newlines='\n'ìœ¼ë¡œ ë‹¨ë… \r ë¬¸ì œ í•´ê²°
        processed_pairs = 0
        skipped_pairs = 0
        raw_src_sentences = []
        raw_tgt_sentences = []

        with open(
            self.src_file, "r", encoding="utf-8", errors="replace", newline="\n"
        ) as f_src, open(
            self.tgt_file, "r", encoding="utf-8", errors="replace", newline="\n"
        ) as f_tgt:

            for line_num, (src_line, tgt_line) in enumerate(zip(f_src, f_tgt), 1):
                src_line = src_line.strip()
                tgt_line = tgt_line.strip()

                # ğŸš¨ ë¹ˆ ë¼ì¸ ìŒì€ ëª¨ë‘ ê±´ë„ˆë›°ê¸°
                if not src_line and not tgt_line:
                    skipped_pairs += 1
                    continue
                elif not src_line or not tgt_line:
                    # í•œìª½ë§Œ ë¹„ì–´ìˆìœ¼ë©´ ê²½ê³ í•˜ê³  ê±´ë„ˆë›°ê¸°
                    if skipped_pairs < 10:  # ì²˜ìŒ 10ê°œë§Œ ë¡œê¹…
                        logger.warning(f"Line {line_num}: One-sided empty - skipping")
                    skipped_pairs += 1
                    continue

                # ê¸¸ì´ ì œí•œ ë° ë¹ˆ ë¼ì¸ í•„í„°ë§
                if (
                    len(src_line) > 0
                    and len(tgt_line) > 0
                    and len(src_line) <= self.max_length
                    and len(tgt_line) <= self.max_length
                ):
                    raw_src_sentences.append(src_line)
                    raw_tgt_sentences.append(tgt_line)
                    processed_pairs += 1
                else:
                    skipped_pairs += 1

        logger.info(f"ğŸ“Š Raw data loading completed:")
        logger.info(f"  Binary line count: {src_line_count:,}")
        logger.info(f"  Raw processed pairs: {processed_pairs:,}")
        logger.info(f"  Raw skipped pairs: {skipped_pairs:,}")

        # ğŸ§¹ ë°ì´í„° í´ë¦¬ë‹ ì ìš© (ì„¤ì •ì— ë”°ë¼)
        apply_cleaning = getattr(self, "apply_cleaning", True)  # ê¸°ë³¸ê°’: True

        if apply_cleaning:
            from src.data_loader import clean_sentence_pairs

            logger.info(f"ğŸ§¹ Applying Tensor2Tensor data cleaning rules...")
            cleaned_src, cleaned_tgt = clean_sentence_pairs(
                raw_src_sentences, raw_tgt_sentences
            )
        else:
            logger.info(f"â­ï¸ Skipping data cleaning (disabled in config)")
            cleaned_src, cleaned_tgt = raw_src_sentences, raw_tgt_sentences

        # ìµœì¢… ë°ì´í„° ìŒ ìƒì„±
        for src_text, tgt_text in zip(cleaned_src, cleaned_tgt):
            data_pairs.append((src_text, tgt_text))

        logger.info(f"âœ… Final data loading completed:")
        logger.info(f"  Final pairs: {len(data_pairs):,}")
        logger.info(
            f"  Overall success rate: {len(data_pairs)/src_line_count*100:.1f}%"
        )

        return data_pairs

    def __len__(self):
        return len(self.data_pairs)

    def __getitem__(self, idx):
        src_text, tgt_text = self.data_pairs[idx]

        # BPEë¡œ í† í°ì„ IDë¡œ ë³€í™˜
        src_ids = self.vocab.encode(src_text)
        tgt_ids = self.vocab.encode(tgt_text)

        # BOS/EOS í† í° ì¶”ê°€
        tgt_input = [self.vocab.special_tokens["<BOS>"]] + tgt_ids
        tgt_output = tgt_ids + [self.vocab.special_tokens["<EOS>"]]

        return {
            "src": torch.tensor(src_ids, dtype=torch.long),
            "tgt": torch.tensor(tgt_input, dtype=torch.long),
            "tgt_y": torch.tensor(tgt_output, dtype=torch.long),
            "src_len": len(src_ids),
            "tgt_len": len(tgt_input),
        }

