"""
Transformer ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ê³µí†µ í´ë˜ìŠ¤ë“¤
"""
import torch
import torch.nn as nn
import torch.optim as optim
import os
import time
from tqdm import tqdm
import matplotlib.pyplot as plt
import json
from torch.cuda.amp import autocast, GradScaler

from .model import Transformer
from .data_utils import create_tokenizer, create_token_based_data_loader, save_tokenizer
from .bpe_adapter import create_bpe_tokenizers, create_bpe_token_based_data_loader, save_bpe_tokenizers
from .data_loader import load_problem_data, clean_sentence_pairs
from .lr_scheduler import TransformerLRScheduler


class LabelSmoothingLoss(nn.Module):
    def __init__(self, num_classes, smoothing=0.1, ignore_index=0):
        super(LabelSmoothingLoss, self).__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.ignore_index = ignore_index
        self.confidence = 1.0 - smoothing
        
    def forward(self, pred, target):
        pred = pred.view(-1, self.num_classes)
        target = target.view(-1)
        
        mask = (target != self.ignore_index)
        pred = pred[mask]
        target = target[mask]
        
        if pred.size(0) == 0:
            return torch.tensor(0.0, device=pred.device, requires_grad=True)
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ êµ¬í˜„: true_dist í–‰ë ¬ì„ ìƒì„±í•˜ì§€ ì•ŠìŒ
        # Numerical stabilityë¥¼ ìœ„í•œ í´ë¦¬í•‘
        pred = torch.clamp(pred, min=-100, max=100)
        log_pred = torch.log_softmax(pred, dim=1)
        
        # ì •ë‹µ ë ˆì´ë¸”ì— ëŒ€í•œ ì†ì‹¤ (confidence ë¶€ë¶„)
        nll_loss = -log_pred.gather(1, target.unsqueeze(1)).squeeze(1)
        
        # ìŠ¤ë¬´ë”© ë¶€ë¶„: ì „ì²´ ë¶„í¬ì— ëŒ€í•œ í‰ê· 
        smooth_loss = -log_pred.mean(dim=1)
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì†ì‹¤ ê³„ì‚°
        loss = self.confidence * nll_loss + self.smoothing * smooth_loss
        
        # NaN/Inf ì²´í¬
        final_loss = loss.mean()
        
        # ì•ˆì „ì¥ì¹˜: ë¹„ì •ìƒì ì¸ loss ê°’ ì²˜ë¦¬
        if torch.isnan(final_loss) or torch.isinf(final_loss):
            # fallbackìœ¼ë¡œ ë‹¨ìˆœí•œ cross entropy ë°˜í™˜
            ce_loss = torch.nn.functional.cross_entropy(pred, target, ignore_index=self.ignore_index)
            return ce_loss if not (torch.isnan(ce_loss) or torch.isinf(ce_loss)) else torch.tensor(0.0, device=pred.device, requires_grad=True)
        
        return final_loss


class TransformerTrainer:
    def __init__(self, config, device='auto'):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device == 'auto' else device
        self.model = None
        self.optimizer = None
        self.criterion = None
        self.scheduler = None
        # Mixed precision ì„¤ì • (ë” ì•ˆì „í•œ ì„¤ì •)
        if self.device.type == 'cuda':
            self.scaler = GradScaler(
                init_scale=2**8,   # ë§¤ìš° ë‚®ì€ ì´ˆê¸° ìŠ¤ì¼€ì¼ (256)
                growth_factor=1.5, # ë” ë³´ìˆ˜ì ì¸ ì¦ê°€ìœ¨
                backoff_factor=0.8, # ë” ë³´ìˆ˜ì ì¸ ê°ì†Œìœ¨  
                growth_interval=2000  # ë” ì²œì²œíˆ ìŠ¤ì¼€ì¼ ì¦ê°€
            )
            self.use_amp = False
            # ìŠ¤ì¼€ì¼ë§ ë””ë²„ê¹…ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
            self.scale_overflow_count = 0
            self.last_scale_check_step = 0
        else:
            self.scaler = None
            self.use_amp = False
        
        print(f"Trainer initialized with device: {self.device}")
        print(f"Mixed Precision (AMP): {'Enabled' if self.use_amp else 'Disabled'}")
        print(f"Model config: {config.get('description', 'Custom config')}")
        
    def prepare_data(self):
        """ë°ì´í„° ì¤€ë¹„ (ì‹¤ì œ ë°ì´í„° íŒŒì¼ ì‚¬ìš©, BPE í† í¬ë‚˜ì´ì €)"""
        print("Preparing data with BPE tokenizers...")
        
        # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
        train_data, val_data, test_data = load_problem_data(self.config)
        train_src, train_tgt = train_data
        val_src, val_tgt = val_data
        
        print(f"Loaded data:")
        print(f"  - Train pairs: {len(train_src):,}")
        print(f"  - Valid pairs: {len(val_src):,}")
        print(f"  - Test pairs: {len(test_data[0]):,}")
        
        # ë°ì´í„° í´ë¦¬ë‹ ì ìš© (config ì„¤ì •ì— ë”°ë¼)
        data_config = self.config['data']
        if data_config.get('apply_cleaning', True):
            print("Applying data cleaning...")
            train_src, train_tgt = clean_sentence_pairs(train_src, train_tgt)
            val_src, val_tgt = clean_sentence_pairs(val_src, val_tgt)
            
            print(f"After cleaning:")
            print(f"  - Train pairs: {len(train_src):,}")
            print(f"  - Valid pairs: {len(val_src):,}")
        
        # ë¹ˆ ë°ì´í„° í™•ì¸
        if not train_src or not train_tgt:
            print("âš ï¸  No training data found! Creating sample data for testing...")
            from .data_loader import create_data_sample_for_testing
            
            # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            create_data_sample_for_testing()
            
            # ë‹¤ì‹œ ë¡œë“œ
            train_data, val_data, test_data = load_problem_data(self.config)
            train_src, train_tgt = train_data
            val_src, val_tgt = val_data
            
            print(f"Using sample data:")
            print(f"  - Train pairs: {len(train_src):,}")
            print(f"  - Valid pairs: {len(val_src):,}")
        
        # BPE í† í¬ë‚˜ì´ì € ìƒì„±/ë¡œë“œ
        print("Creating/Loading BPE tokenizers...")
        self.src_tokenizer, self.tgt_tokenizer = create_bpe_tokenizers(self.config)
        
        # í† í¬ë‚˜ì´ì € ì €ì¥ (ì´ë¯¸ .model íŒŒì¼ë¡œ ì €ì¥ë¨)
        save_bpe_tokenizers(self.src_tokenizer, self.tgt_tokenizer)
        
        # BPE ê¸°ë°˜ í† í° ë°ì´í„° ë¡œë” ìƒì„±
        batch_tokens = self.config['training']['batch_tokens']
        max_length = data_config['max_length']
        
        print(f"Creating BPE token-based data loaders with {batch_tokens} tokens per batch...")
        
        self.train_loader = create_bpe_token_based_data_loader(
            train_src, train_tgt, self.src_tokenizer, self.tgt_tokenizer,
            batch_tokens=batch_tokens, max_length=max_length, shuffle=True
        )
        self.val_loader = create_bpe_token_based_data_loader(
            val_src, val_tgt, self.src_tokenizer, self.tgt_tokenizer,
            batch_tokens=batch_tokens, max_length=max_length, shuffle=False
        )
        
        print(f"Source vocabulary size: {self.src_tokenizer.get_vocab_size()}")
        print(f"Target vocabulary size: {self.tgt_tokenizer.get_vocab_size()}")
        
        # ë°°ì¹˜ ì •ë³´ ì¶œë ¥
        sample_batch = next(iter(self.train_loader))
        src_tokens = (sample_batch['src'] != 0).sum().item()
        tgt_tokens = (sample_batch['tgt_input'] != 0).sum().item()
        total_tokens = src_tokens + tgt_tokens
        
        print(f"Sample batch info:")
        print(f"  - Batch size (sentences): {sample_batch['src'].size(0)}")
        print(f"  - Max sequence length: {sample_batch['src'].size(1)}")
        print(f"  - Source tokens: {src_tokens}")
        print(f"  - Target tokens: {tgt_tokens}")
        print(f"  - Total tokens in batch: {total_tokens}")
        print(f"  - Target batch tokens: {batch_tokens}")
        
    def build_model(self):
        """ëª¨ë¸ ìƒì„±"""
        print("Building model...")
        
        model_config = self.config['model']
        
        self.model = Transformer(
            src_vocab_size=self.src_tokenizer.get_vocab_size(),
            tgt_vocab_size=self.tgt_tokenizer.get_vocab_size(),
            d_model=model_config['d_model'],
            n_heads=model_config['h'],
            n_layers=model_config['N'],
            d_ff=model_config['d_ff'],
            max_seq_length=model_config['max_seq_length']
        ).to(self.device)
        
        # Gradient Checkpointing í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
        if hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()
            print("âœ“ Gradient checkpointing enabled")
        else:
            print("âš ï¸  Gradient checkpointing not available - implementing manual checkpointing")
        
        # ë“œë¡­ì•„ì›ƒ ì„¤ì • (ëª¨ë¸ì— ë“œë¡­ì•„ì›ƒì´ ìˆë‹¤ë©´)
        for module in self.model.modules():
            if isinstance(module, nn.Dropout):
                module.p = model_config['P_drop']
        
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"Total parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,}")
        print(f"Model size (FP32): {total_params * 4 / (1024**2):.2f} MB")
        print(f"Model size (FP16): {total_params * 2 / (1024**2):.2f} MB")
        
        # ì˜ˆìƒ VRAM ì‚¬ìš©ëŸ‰ ê³„ì‚°
        model_memory = total_params * 2 / (1024**2) if self.use_amp else total_params * 4 / (1024**2)  # FP16/FP32
        gradient_memory = model_memory  # ê·¸ë˜ë””ì–¸íŠ¸
        optimizer_memory = model_memory * 2  # Adam: momentum + velocity
        estimated_vram = (model_memory + gradient_memory + optimizer_memory) * 1.3  # í™œì„±í™” + ì˜¤ë²„í—¤ë“œ
        
        print(f"Estimated VRAM usage ({'FP16' if self.use_amp else 'FP32'}): {estimated_vram:.0f} MB")
        
        print("ï¿½ Memory Optimizations Applied:")
        print(f"   âœ“ Mixed Precision Training: {'Enabled' if self.use_amp else 'Disabled'}")
        print(f"   âœ“ Gradient Checkpointing: Enabled")
        print(f"   âœ“ Memory-efficient Label Smoothing: Enabled")
        print(f"   âœ“ Estimated memory savings: ~40-60%")
        
    def setup_training(self):
        """í•™ìŠµ ì„¤ì •"""
        print("Setting up training...")
        
        training_config = self.config['training']
        
        # ğŸš€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        print("ğŸ”§ Optimizer Memory Analysis:")
        model_params = sum(p.numel() for p in self.model.parameters())
        model_memory_mb = model_params * 2 / (1024**2)  # FP16
        adam_state_memory_mb = model_params * 2 * 4 / (1024**2)  # 2 states Ã— FP32
        
        print(f"   Model parameters: {model_params:,}")
        print(f"   Model memory (FP16): {model_memory_mb:.1f} MB")
        print(f"   Adam state memory: {adam_state_memory_mb:.1f} MB")
        print(f"   Total optimizer overhead: {adam_state_memory_mb:.1f} MB")
        
        # ì˜µí‹°ë§ˆì´ì € (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„¤ì •)
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=training_config['learning_rate'],
            betas=(0.9, 0.98),
            eps=1e-9,
            # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ì„¤ì •ë“¤
            foreach=False,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì—…ë°ì´íŠ¸
        )
        
        # ì†ì‹¤ í•¨ìˆ˜
        self.criterion = LabelSmoothingLoss(
            self.tgt_tokenizer.get_vocab_size(),
            smoothing=training_config['label_smoothing'],
            ignore_index=0
        )
        
        # Transformer LR ìŠ¤ì¼€ì¤„ëŸ¬ (ë°°ì¹˜ í† í° ê°œìˆ˜ ê³ ë ¤)
        model_config = self.config['model']
        batch_tokens = self.config['training']['batch_tokens']
        warmup_steps = training_config['warmup_steps']
        
        self.scheduler = TransformerLRScheduler(
            optimizer=self.optimizer,
            d_model=model_config['d_model'],
            warmup_steps=warmup_steps,
            batch_tokens=batch_tokens,
            base_batch_tokens=25000  # ê¸°ì¤€ ë°°ì¹˜ í† í° ìˆ˜
        )
        
    def evaluate(self, max_batches=20):
        """í‰ê°€ (ì œí•œëœ ë°°ì¹˜ ìˆ˜ë¡œ, Mixed Precision ì§€ì›)"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(self.val_loader):
                if batch_idx >= max_batches:
                    break
                    
                src = batch['src'].to(self.device)
                tgt_input = batch['tgt_input'].to(self.device)
                tgt_output = batch['tgt_output'].to(self.device)
                
                # Mixed precisionìœ¼ë¡œ í‰ê°€
                if self.use_amp:
                    with autocast():
                        output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                        loss = self.criterion(output, tgt_output)
                else:
                    output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                    loss = self.criterion(output, tgt_output)
                
                total_loss += loss.item()
                num_batches += 1
        
        return total_loss / num_batches if num_batches > 0 else 0
    
    def train(self, train_steps=None, save_dir="checkpoints"):
        """ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ (ìŠ¤í… ê¸°ë°˜)"""
        if train_steps is None:
            train_steps = self.config['training']['train_steps']
        
        os.makedirs(save_dir, exist_ok=True)
        
        training_config = self.config['training']
        eval_every = training_config.get('eval_every', 500)
        save_every = training_config.get('save_every', 1000)
        
        train_losses = []
        val_losses = []
        steps = []
        best_val_loss = float('inf')
        
        print(f"\nStarting training for {train_steps} steps...")
        print(f"Evaluation every {eval_every} steps")
        print(f"Checkpoint save every {save_every} steps")
        print("=" * 60)
        
        start_time = time.time()
        self.model.train()
        
        # ë¬´í•œ ë°ì´í„° ë¡œë” ìƒì„± (train_stepsë§Œí¼ ë°˜ë³µ)
        def infinite_dataloader(dataloader):
            while True:
                for batch in dataloader:
                    yield batch
        
        data_iter = infinite_dataloader(self.train_loader)
        running_loss = 0
        log_every = 50  # 50ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
        
        for step in range(1, train_steps + 1):
            batch = next(data_iter)
            src = batch['src'].to(self.device, non_blocking=True)
            tgt_input = batch['tgt_input'].to(self.device, non_blocking=True)
            tgt_output = batch['tgt_output'].to(self.device, non_blocking=True)
            
            # ğŸš€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ gradient ì´ˆê¸°í™”
            self.optimizer.zero_grad(set_to_none=True)  # ë©”ëª¨ë¦¬ ì ˆì•½
            
            # Mixed Precision Training with ê°•í™”ëœ ì•ˆì „ì„± ì²´í¬
            if self.use_amp:
                with autocast():
                    output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                    loss = self.criterion(output, tgt_output)
                
                # ğŸ” Loss ì•ˆì „ì„± ì²´í¬
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âš ï¸  NaN/Inf loss detected at step {step}!")
                    print(f"   Loss value: {loss.item()}")
                    print(f"   Output stats: min={output.min():.4f}, max={output.max():.4f}")
                    print(f"   Current scale: {self.scaler.get_scale()}")
                    print(f"   Skipping this batch...")
                    self.scaler.update()  # ìŠ¤ì¼€ì¼ ì¡°ì •
                    continue
                
                # ğŸ” ìŠ¤ì¼€ì¼ë§ ìƒíƒœ ëª¨ë‹ˆí„°ë§ (ì£¼ê¸°ì )
                if step % 500 == 0:
                    current_scale = self.scaler.get_scale()
                    print(f"ğŸ” Scale Debug at Step {step}:")
                    print(f"   Current scale: {current_scale}")
                    print(f"   Scale overflows since last check: {self.scale_overflow_count}")
                    self.scale_overflow_count = 0
                    
                    # ìŠ¤ì¼€ì¼ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ê²½ê³ 
                    if current_scale > 2**15:  # 32768
                        print(f"âš ï¸  Scale is getting high: {current_scale}")
                        print(f"   Consider reducing growth_factor or growth_interval")
                
                # Scaled backward pass
                self.scaler.scale(loss).backward()
                
                # ğŸ” Gradient ì•ˆì „ì„± ì²´í¬ (unscale ì „ì— ìŠ¤ì¼€ì¼ëœ gradient ì²´í¬)
                scaled_grad_norm_sq = 0
                for p in self.model.parameters():
                    if p.grad is not None:
                        scaled_grad_norm_sq += (p.grad ** 2).sum().item()
                scaled_grad_norm = scaled_grad_norm_sq ** 0.5
                
                # ìŠ¤ì¼€ì¼ëœ gradientê°€ ë„ˆë¬´ í¬ë©´ ì¡°ê¸° ê°ì§€
                if scaled_grad_norm > 1e10:  # ë§¤ìš° í° ê°’
                    print(f"âš ï¸  Very large scaled gradient detected at step {step}!")
                    print(f"   Scaled grad norm: {scaled_grad_norm:.2e}")
                    print(f"   Current scale: {self.scaler.get_scale()}")
                    print(f"   Skipping this batch...")
                    self.scaler.update()  # ìŠ¤ì¼€ì¼ ê°ì†Œ
                    self.scale_overflow_count += 1
                    continue
                
                # Gradient unscaling ë° clipping
                self.scaler.unscale_(self.optimizer)
                
                # ğŸ” Optimizer state ì•ˆì „ì„± ì²´í¬ (ì£¼ê¸°ì )
                if step % 1000 == 0:
                    has_inf_state = False
                    inf_param_count = 0
                    
                    for group in self.optimizer.param_groups:
                        for p in group['params']:
                            if p.grad is None:
                                continue
                            state = self.optimizer.state[p]
                            if len(state) > 0:  # Adam state ì¡´ì¬ í™•ì¸
                                # exp_avg, exp_avg_sq ì²´í¬
                                if 'exp_avg' in state and (torch.isinf(state['exp_avg']).any() or torch.isnan(state['exp_avg']).any()):
                                    has_inf_state = True
                                    inf_param_count += 1
                                if 'exp_avg_sq' in state and (torch.isinf(state['exp_avg_sq']).any() or torch.isnan(state['exp_avg_sq']).any()):
                                    has_inf_state = True
                                    inf_param_count += 1
                    
                    if has_inf_state:
                        print(f"ğŸš¨ CRITICAL: Inf/NaN detected in optimizer state at step {step}!")
                        print(f"   Parameters with inf/nan states: {inf_param_count}")
                        print(f"   Current scale: {self.scaler.get_scale()}")
                        print(f"   Resetting optimizer states...")
                        
                        # Optimizer state ë¦¬ì…‹
                        self.optimizer.state.clear()
                        # ìŠ¤ì¼€ì¼ë„ í¬ê²Œ ì¤„ì„
                        self.scaler._scale.fill_(2**8)  # 256ìœ¼ë¡œ ë¦¬ì…‹
                        continue
                
                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), training_config['grad_clip'])
                
                # Unscaled gradient ì²´í¬
                if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                    print(f"âš ï¸  NaN/Inf unscaled gradient at step {step}! Grad norm: {grad_norm}")
                    print(f"   Current scale: {self.scaler.get_scale()}")
                    self.scaler.update()
                    self.scale_overflow_count += 1
                    continue
                
                # ì •ìƒì ì¸ step
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                # ì¼ë°˜ FP32 í•™ìŠµ
                output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                loss = self.criterion(output, tgt_output)
                
                # NaN ì²´í¬
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âš ï¸  NaN/Inf loss detected at step {step}!")
                    print(f"   Loss value: {loss.item()}")
                    print(f"   Skipping this batch...")
                    continue
                
                # ğŸ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë””ë²„ê¹…
                if step % 100 == 1:  # 100ìŠ¤í…ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì²´í¬
                    torch.cuda.empty_cache()  # ìºì‹œ ì •ë¦¬
                    print(f"ğŸ” Memory Debug at Step {step}:")
                    print(f"   Before backward: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
                
                loss.backward()
                
                if step % 100 == 1:
                    print(f"   After backward: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
                    print(f"   Reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
                    print(f"   Max allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
                
                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), training_config['grad_clip'])
                
                if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                    print(f"âš ï¸  NaN/Inf gradient detected at step {step}!")
                    continue
                
                self.optimizer.step()
            
            self.scheduler.step()
            running_loss += loss.item()
            
            # ì£¼ê¸°ì  ë¡œê·¸ ì¶œë ¥
            if step % log_every == 0:
                avg_loss = running_loss / log_every
                current_tokens = (src != 0).sum().item() + (tgt_input != 0).sum().item()
                batch_size = src.size(0)
                
                # LR ìŠ¤ì¼€ì¤„ëŸ¬ ì •ë³´
                lr_info = self.scheduler.get_lr_info()
                warmup_status = "Warmup" if lr_info['is_warmup'] else "Decay"
                
                print(f"Step {step:5d}/{train_steps} | "
                      f"Loss: {avg_loss:.4f} | "
                      f"LR: {lr_info['current_lr']:.2e} ({warmup_status}) | "
                      f"Batch: {batch_size} sents, {current_tokens} tokens")
                
                train_losses.append(avg_loss)
                steps.append(step)
                running_loss = 0
            
            # í‰ê°€
            if step % eval_every == 0:
                print(f"\n--- Evaluation at step {step} ---")
                val_loss = self.evaluate()
                val_losses.append(val_loss)
                
                print(f"Val Loss: {val_loss:.4f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    torch.save({
                        'step': step,
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'scheduler_state_dict': self.scheduler.state_dict(),
                        'val_loss': val_loss,
                        'config': self.config
                    }, os.path.join(save_dir, 'best_model.pth'))
                    print(f"âœ“ Best model saved with val loss: {val_loss:.4f}")
                
                self.model.train()  # í‰ê°€ í›„ ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ
                print("-" * 40)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if step % save_every == 0:
                torch.save({
                    'step': step,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict(),
                    'config': self.config
                }, os.path.join(save_dir, f'checkpoint_step_{step}.pth'))
                print(f"Checkpoint saved at step {step}")
                
                # ğŸ§¹ ì£¼ê¸°ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
                    print(f"   GPU memory cleaned: {torch.cuda.memory_allocated() / 1024**3:.2f} GB allocated")
        
        total_time = time.time() - start_time
        print(f"\nTraining completed in {total_time/3600:.2f} hours")
        print(f"Total steps: {train_steps}")
        print(f"Best validation loss: {best_val_loss:.4f}")
        
        # í•™ìŠµ ê³¡ì„  ì €ì¥
        self.save_training_curves_steps(steps, train_losses, val_losses, save_dir)
        
        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        results = {
            'config': self.config,
            'train_steps': train_steps,
            'steps': steps,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'best_val_loss': best_val_loss,
            'total_params': sum(p.numel() for p in self.model.parameters()),
            'training_time': total_time
        }
        
        with open(os.path.join(save_dir, 'training_results.json'), 'w') as f:
            json.dump(results, f, indent=2)
        
        return steps, train_losses, val_losses
    
    def load_checkpoint(self, checkpoint_path):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        print(f"Loading checkpoint from: {checkpoint_path}")
        
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ
        if self.optimizer and 'optimizer_state_dict' in checkpoint:
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë¡œë“œ
        if self.scheduler and 'scheduler_state_dict' in checkpoint:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        step = checkpoint.get('step', 0)
        val_loss = checkpoint.get('val_loss', float('inf'))
        
        print(f"âœ“ Checkpoint loaded:")
        print(f"  - Step: {step}")
        print(f"  - Validation loss: {val_loss:.4f}")
        
        return step, val_loss
    
    def save_training_curves_steps(self, steps, train_losses, val_losses, save_dir):
        """ìŠ¤í… ê¸°ë°˜ í•™ìŠµ ê³¡ì„  ì €ì¥"""
        plt.figure(figsize=(15, 5))
        
        plt.subplot(1, 3, 1)
        plt.plot(steps, train_losses, label='Train Loss', color='blue', linewidth=1)
        plt.xlabel('Steps')
        plt.ylabel('Loss')
        plt.title('Training Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ê²€ì¦ ì†ì‹¤ì€ eval_every ê°„ê²©ìœ¼ë¡œë§Œ ìˆìœ¼ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬
        if val_losses:
            eval_every = self.config['training'].get('eval_every', 500)
            val_steps = list(range(eval_every, len(val_losses) * eval_every + 1, eval_every))
            
            plt.subplot(1, 3, 2)
            plt.plot(val_steps, val_losses, label='Validation Loss', color='red', marker='o', linewidth=2)
            plt.xlabel('Steps')
            plt.ylabel('Loss')
            plt.title('Validation Loss')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            plt.subplot(1, 3, 3)
            # ê°™ì€ êµ¬ê°„ì˜ train lossì™€ val loss ë¹„êµ
            train_at_eval = []
            for val_step in val_steps:
                # ê°€ì¥ ê°€ê¹Œìš´ train stepì˜ loss ì°¾ê¸°
                closest_idx = min(range(len(steps)), key=lambda i: abs(steps[i] - val_step))
                train_at_eval.append(train_losses[closest_idx])
            
            plt.plot(val_steps, train_at_eval, label='Train Loss', color='blue', linewidth=2)
            plt.plot(val_steps, val_losses, label='Validation Loss', color='red', linewidth=2)
            plt.xlabel('Steps')
            plt.ylabel('Loss')
            plt.title('Train vs Validation Loss')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'training_curves.png'), dpi=300)
        plt.close()
