"""
Transformer ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ê³µí†µ í´ë˜ìŠ¤ë“¤
"""
import torch
import torch.nn as nn
import torch.optim as optim
import os
import time
from tqdm import tqdm
import matplotlib.pyplot as plt
import json
from torch.cuda.amp import autocast, GradScaler

from .model import Transformer
from .data_utils import create_tokenizer, create_token_based_data_loader, save_tokenizer
from .bpe_adapter import create_bpe_tokenizers, create_bpe_token_based_data_loader, save_bpe_tokenizers
from .data_loader import load_problem_data, clean_sentence_pairs


class LabelSmoothingLoss(nn.Module):
    def __init__(self, num_classes, smoothing=0.1, ignore_index=0):
        super(LabelSmoothingLoss, self).__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.ignore_index = ignore_index
        self.confidence = 1.0 - smoothing
        
    def forward(self, pred, target):
        pred = pred.view(-1, self.num_classes)
        target = target.view(-1)
        
        mask = (target != self.ignore_index)
        pred = pred[mask]
        target = target[mask]
        
        if pred.size(0) == 0:
            return torch.tensor(0.0, device=pred.device, requires_grad=True)
        
        true_dist = torch.zeros_like(pred)
        true_dist.fill_(self.smoothing / (self.num_classes - 1))
        true_dist.scatter_(1, target.unsqueeze(1), self.confidence)
        
        log_pred = torch.log_softmax(pred, dim=1)
        loss = -torch.sum(true_dist * log_pred, dim=1).mean()
        
        return loss


class TransformerTrainer:
    def __init__(self, config, device='auto'):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device == 'auto' else device
        self.model = None
        self.optimizer = None
        self.criterion = None
        self.scheduler = None
        self.scaler = GradScaler() if self.device.type == 'cuda' else None  # Mixed precisionìš©
        self.use_amp = self.device.type == 'cuda'  # CUDAì¼ ë•Œë§Œ AMP ì‚¬ìš©
        
        print(f"Trainer initialized with device: {self.device}")
        print(f"Mixed Precision (AMP): {'Enabled' if self.use_amp else 'Disabled'}")
        print(f"Model config: {config.get('description', 'Custom config')}")
        
    def prepare_data(self):
        """ë°ì´í„° ì¤€ë¹„ (ì‹¤ì œ ë°ì´í„° íŒŒì¼ ì‚¬ìš©, BPE í† í¬ë‚˜ì´ì €)"""
        print("Preparing data with BPE tokenizers...")
        
        # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
        train_data, val_data, test_data = load_problem_data(self.config)
        train_src, train_tgt = train_data
        val_src, val_tgt = val_data
        
        print(f"Loaded data:")
        print(f"  - Train pairs: {len(train_src):,}")
        print(f"  - Valid pairs: {len(val_src):,}")
        print(f"  - Test pairs: {len(test_data[0]):,}")
        
        # ë°ì´í„° í´ë¦¬ë‹ ì ìš© (config ì„¤ì •ì— ë”°ë¼)
        data_config = self.config['data']
        if data_config.get('apply_cleaning', True):
            print("Applying data cleaning...")
            train_src, train_tgt = clean_sentence_pairs(train_src, train_tgt)
            val_src, val_tgt = clean_sentence_pairs(val_src, val_tgt)
            
            print(f"After cleaning:")
            print(f"  - Train pairs: {len(train_src):,}")
            print(f"  - Valid pairs: {len(val_src):,}")
        
        # ë¹ˆ ë°ì´í„° í™•ì¸
        if not train_src or not train_tgt:
            print("âš ï¸  No training data found! Creating sample data for testing...")
            from .data_loader import create_data_sample_for_testing
            
            # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            create_data_sample_for_testing()
            
            # ë‹¤ì‹œ ë¡œë“œ
            train_data, val_data, test_data = load_problem_data(self.config)
            train_src, train_tgt = train_data
            val_src, val_tgt = val_data
            
            print(f"Using sample data:")
            print(f"  - Train pairs: {len(train_src):,}")
            print(f"  - Valid pairs: {len(val_src):,}")
        
        # BPE í† í¬ë‚˜ì´ì € ìƒì„±/ë¡œë“œ
        print("Creating/Loading BPE tokenizers...")
        self.src_tokenizer, self.tgt_tokenizer = create_bpe_tokenizers(self.config)
        
        # í† í¬ë‚˜ì´ì € ì €ì¥ (ì´ë¯¸ .model íŒŒì¼ë¡œ ì €ì¥ë¨)
        save_bpe_tokenizers(self.src_tokenizer, self.tgt_tokenizer)
        
        # BPE ê¸°ë°˜ í† í° ë°ì´í„° ë¡œë” ìƒì„±
        batch_tokens = self.config['training']['batch_tokens']
        max_length = data_config['max_length']
        
        print(f"Creating BPE token-based data loaders with {batch_tokens} tokens per batch...")
        
        self.train_loader = create_bpe_token_based_data_loader(
            train_src, train_tgt, self.src_tokenizer, self.tgt_tokenizer,
            batch_tokens=batch_tokens, max_length=max_length, shuffle=True
        )
        self.val_loader = create_bpe_token_based_data_loader(
            val_src, val_tgt, self.src_tokenizer, self.tgt_tokenizer,
            batch_tokens=batch_tokens, max_length=max_length, shuffle=False
        )
        
        print(f"Source vocabulary size: {self.src_tokenizer.get_vocab_size()}")
        print(f"Target vocabulary size: {self.tgt_tokenizer.get_vocab_size()}")
        
        # ë°°ì¹˜ ì •ë³´ ì¶œë ¥
        sample_batch = next(iter(self.train_loader))
        src_tokens = (sample_batch['src'] != 0).sum().item()
        tgt_tokens = (sample_batch['tgt_input'] != 0).sum().item()
        total_tokens = src_tokens + tgt_tokens
        
        print(f"Sample batch info:")
        print(f"  - Batch size (sentences): {sample_batch['src'].size(0)}")
        print(f"  - Max sequence length: {sample_batch['src'].size(1)}")
        print(f"  - Source tokens: {src_tokens}")
        print(f"  - Target tokens: {tgt_tokens}")
        print(f"  - Total tokens in batch: {total_tokens}")
        print(f"  - Target batch tokens: {batch_tokens}")
        
    def build_model(self):
        """ëª¨ë¸ ìƒì„±"""
        print("Building model...")
        
        model_config = self.config['model']
        
        self.model = Transformer(
            src_vocab_size=self.src_tokenizer.get_vocab_size(),
            tgt_vocab_size=self.tgt_tokenizer.get_vocab_size(),
            d_model=model_config['d_model'],
            n_heads=model_config['h'],
            n_layers=model_config['N'],
            d_ff=model_config['d_ff'],
            max_seq_length=model_config['max_seq_length']
        ).to(self.device)
        
        # ë“œë¡­ì•„ì›ƒ ì„¤ì • (ëª¨ë¸ì— ë“œë¡­ì•„ì›ƒì´ ìˆë‹¤ë©´)
        for module in self.model.modules():
            if isinstance(module, nn.Dropout):
                module.p = model_config['P_drop']
        
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"Total parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,}")
        print(f"Model size (FP32): {total_params * 4 / (1024**2):.2f} MB")
        print(f"Model size (FP16): {total_params * 2 / (1024**2):.2f} MB")
        
        # ì˜ˆìƒ VRAM ì‚¬ìš©ëŸ‰ ê³„ì‚°
        model_memory = total_params * 2 / (1024**2) if self.use_amp else total_params * 4 / (1024**2)  # FP16/FP32
        gradient_memory = model_memory  # ê·¸ë˜ë””ì–¸íŠ¸
        optimizer_memory = model_memory * 2  # Adam: momentum + velocity
        estimated_vram = (model_memory + gradient_memory + optimizer_memory) * 1.3  # í™œì„±í™” + ì˜¤ë²„í—¤ë“œ
        
        print(f"Estimated VRAM usage ({'FP16' if self.use_amp else 'FP32'}): {estimated_vram:.0f} MB")
        if self.use_amp:
            print("ğŸ’¡ Mixed Precision Training enabled - significant VRAM savings!")
        else:
            print("ğŸ’¡ Consider enabling CUDA for Mixed Precision Training to save VRAM")
        
        # ì˜ˆìƒ VRAM ì‚¬ìš©ëŸ‰ ê³„ì‚° (ëŒ€ëµì )
        model_memory = total_params * 4 / (1024**2)  # FP32 ê¸°ì¤€
        gradient_memory = model_memory  # ê·¸ë˜ë””ì–¸íŠ¸
        optimizer_memory = model_memory * 2  # Adam: momentum + velocity
        estimated_vram = (model_memory + gradient_memory + optimizer_memory) * 1.5  # í™œì„±í™” + ì˜¤ë²„í—¤ë“œ
        
        print(f"Estimated VRAM usage: {estimated_vram:.0f} MB (excluding batch data)")
        print("ğŸ’¡ VRAM optimization tips:")
        print("   - Use gradient checkpointing: model.gradient_checkpointing_enable()")
        print("   - Use mixed precision: torch.cuda.amp.autocast()")
        print("   - Reduce batch_tokens size")
        print("   - Use gradient accumulation")
        
    def setup_training(self):
        """í•™ìŠµ ì„¤ì •"""
        print("Setting up training...")
        
        training_config = self.config['training']
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=training_config['learning_rate'],
            betas=(0.9, 0.98),
            eps=1e-9
        )
        
        # ì†ì‹¤ í•¨ìˆ˜
        self.criterion = LabelSmoothingLoss(
            self.tgt_tokenizer.get_vocab_size(),
            smoothing=training_config['label_smoothing'],
            ignore_index=0
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ (Warmup)
        warmup_steps = training_config['warmup_steps']
        def lr_lambda(step):
            if step == 0:
                return 0
            return min(step ** (-0.5), step * warmup_steps ** (-1.5))
        
        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)
        
    def evaluate(self, max_batches=20):
        """í‰ê°€ (ì œí•œëœ ë°°ì¹˜ ìˆ˜ë¡œ, Mixed Precision ì§€ì›)"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(self.val_loader):
                if batch_idx >= max_batches:
                    break
                    
                src = batch['src'].to(self.device)
                tgt_input = batch['tgt_input'].to(self.device)
                tgt_output = batch['tgt_output'].to(self.device)
                
                # Mixed precisionìœ¼ë¡œ í‰ê°€
                if self.use_amp:
                    with autocast():
                        output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                        loss = self.criterion(output, tgt_output)
                else:
                    output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                    loss = self.criterion(output, tgt_output)
                
                total_loss += loss.item()
                num_batches += 1
        
        return total_loss / num_batches if num_batches > 0 else 0
    
    def train(self, train_steps=None, save_dir="checkpoints"):
        """ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ (ìŠ¤í… ê¸°ë°˜)"""
        if train_steps is None:
            train_steps = self.config['training']['train_steps']
        
        os.makedirs(save_dir, exist_ok=True)
        
        training_config = self.config['training']
        eval_every = training_config.get('eval_every', 500)
        save_every = training_config.get('save_every', 1000)
        
        train_losses = []
        val_losses = []
        steps = []
        best_val_loss = float('inf')
        
        print(f"\nStarting training for {train_steps} steps...")
        print(f"Evaluation every {eval_every} steps")
        print(f"Checkpoint save every {save_every} steps")
        print("=" * 60)
        
        start_time = time.time()
        self.model.train()
        
        # ë¬´í•œ ë°ì´í„° ë¡œë” ìƒì„± (train_stepsë§Œí¼ ë°˜ë³µ)
        def infinite_dataloader(dataloader):
            while True:
                for batch in dataloader:
                    yield batch
        
        data_iter = infinite_dataloader(self.train_loader)
        running_loss = 0
        log_every = 50  # 50ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
        
        for step in range(1, train_steps + 1):
            batch = next(data_iter)
            src = batch['src'].to(self.device)
            tgt_input = batch['tgt_input'].to(self.device)
            tgt_output = batch['tgt_output'].to(self.device)
            
            self.optimizer.zero_grad()
            
            # Mixed Precision Training
            if self.use_amp:
                with autocast():
                    output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                    loss = self.criterion(output, tgt_output)
                
                # Scaled backward pass
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), training_config['grad_clip'])
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                # ì¼ë°˜ FP32 í•™ìŠµ
                output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                loss = self.criterion(output, tgt_output)
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), training_config['grad_clip'])
                self.optimizer.step()
            
            self.scheduler.step()
            running_loss += loss.item()
            
            # ì£¼ê¸°ì  ë¡œê·¸ ì¶œë ¥
            if step % log_every == 0:
                avg_loss = running_loss / log_every
                current_tokens = (src != 0).sum().item() + (tgt_input != 0).sum().item()
                batch_size = src.size(0)
                
                print(f"Step {step:5d}/{train_steps} | "
                      f"Loss: {avg_loss:.4f} | "
                      f"LR: {self.scheduler.get_last_lr()[0]:.2e} | "
                      f"Batch: {batch_size} sents, {current_tokens} tokens")
                
                train_losses.append(avg_loss)
                steps.append(step)
                running_loss = 0
            
            # í‰ê°€
            if step % eval_every == 0:
                print(f"\n--- Evaluation at step {step} ---")
                val_loss = self.evaluate()
                val_losses.append(val_loss)
                
                print(f"Val Loss: {val_loss:.4f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    torch.save({
                        'step': step,
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'scheduler_state_dict': self.scheduler.state_dict(),
                        'val_loss': val_loss,
                        'config': self.config
                    }, os.path.join(save_dir, 'best_model.pth'))
                    print(f"âœ“ Best model saved with val loss: {val_loss:.4f}")
                
                self.model.train()  # í‰ê°€ í›„ ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ
                print("-" * 40)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if step % save_every == 0:
                torch.save({
                    'step': step,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict(),
                    'config': self.config
                }, os.path.join(save_dir, f'checkpoint_step_{step}.pth'))
                print(f"Checkpoint saved at step {step}")
        
        total_time = time.time() - start_time
        print(f"\nTraining completed in {total_time/3600:.2f} hours")
        print(f"Total steps: {train_steps}")
        print(f"Best validation loss: {best_val_loss:.4f}")
        
        # í•™ìŠµ ê³¡ì„  ì €ì¥
        self.save_training_curves_steps(steps, train_losses, val_losses, save_dir)
        
        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        results = {
            'config': self.config,
            'train_steps': train_steps,
            'steps': steps,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'best_val_loss': best_val_loss,
            'total_params': sum(p.numel() for p in self.model.parameters()),
            'training_time': total_time
        }
        
        with open(os.path.join(save_dir, 'training_results.json'), 'w') as f:
            json.dump(results, f, indent=2)
        
        return steps, train_losses, val_losses
    
    def save_training_curves_steps(self, steps, train_losses, val_losses, save_dir):
        """ìŠ¤í… ê¸°ë°˜ í•™ìŠµ ê³¡ì„  ì €ì¥"""
        plt.figure(figsize=(15, 5))
        
        plt.subplot(1, 3, 1)
        plt.plot(steps, train_losses, label='Train Loss', color='blue', linewidth=1)
        plt.xlabel('Steps')
        plt.ylabel('Loss')
        plt.title('Training Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ê²€ì¦ ì†ì‹¤ì€ eval_every ê°„ê²©ìœ¼ë¡œë§Œ ìˆìœ¼ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬
        if val_losses:
            eval_every = self.config['training'].get('eval_every', 500)
            val_steps = list(range(eval_every, len(val_losses) * eval_every + 1, eval_every))
            
            plt.subplot(1, 3, 2)
            plt.plot(val_steps, val_losses, label='Validation Loss', color='red', marker='o', linewidth=2)
            plt.xlabel('Steps')
            plt.ylabel('Loss')
            plt.title('Validation Loss')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            plt.subplot(1, 3, 3)
            # ê°™ì€ êµ¬ê°„ì˜ train lossì™€ val loss ë¹„êµ
            train_at_eval = []
            for val_step in val_steps:
                # ê°€ì¥ ê°€ê¹Œìš´ train stepì˜ loss ì°¾ê¸°
                closest_idx = min(range(len(steps)), key=lambda i: abs(steps[i] - val_step))
                train_at_eval.append(train_losses[closest_idx])
            
            plt.plot(val_steps, train_at_eval, label='Train Loss', color='blue', linewidth=2)
            plt.plot(val_steps, val_losses, label='Validation Loss', color='red', linewidth=2)
            plt.xlabel('Steps')
            plt.ylabel('Loss')
            plt.title('Train vs Validation Loss')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'training_curves.png'), dpi=300)
        plt.close()
