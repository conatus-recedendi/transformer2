"""
Transformer ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ê³µí†µ í´ë˜ìŠ¤ë“¤
"""

import torch
import torch.nn as nn
import torch.optim as optim
import os
import time
from tqdm import tqdm
import matplotlib.pyplot as plt
import json
from torch.cuda.amp import autocast, GradScaler

from .model import Transformer
from .data_utils import create_tokenizer, create_token_based_data_loader, save_tokenizer
from .bpe_adapter import (
    create_bpe_tokenizers,
    create_bpe_token_based_data_loader,
    save_bpe_tokenizers,
)
from .data_loader import load_problem_data, clean_sentence_pairs
from .lr_scheduler import TransformerLRScheduler


class LabelSmoothingLoss(nn.Module):
    def __init__(self, num_classes, smoothing=0.1, ignore_index=0):
        super(LabelSmoothingLoss, self).__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.ignore_index = ignore_index
        self.confidence = 1.0 - smoothing

    def forward(self, pred, target):
        pred = pred.view(-1, self.num_classes)
        target = target.view(-1)

        mask = target != self.ignore_index
        pred = pred[mask]
        target = target[mask]

        if pred.size(0) == 0:
            return torch.tensor(0.0, device=pred.device, requires_grad=True)

        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ êµ¬í˜„: true_dist í–‰ë ¬ì„ ìƒì„±í•˜ì§€ ì•ŠìŒ
        # Numerical stabilityë¥¼ ìœ„í•œ í´ë¦¬í•‘
        pred = torch.clamp(pred, min=-100, max=100)
        log_pred = torch.log_softmax(pred, dim=1)

        # ì •ë‹µ ë ˆì´ë¸”ì— ëŒ€í•œ ì†ì‹¤ (confidence ë¶€ë¶„)
        nll_loss = -log_pred.gather(1, target.unsqueeze(1)).squeeze(1)

        # ìŠ¤ë¬´ë”© ë¶€ë¶„: ì „ì²´ ë¶„í¬ì— ëŒ€í•œ í‰ê· 
        smooth_loss = -log_pred.mean(dim=1)

        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì†ì‹¤ ê³„ì‚°
        loss = self.confidence * nll_loss + self.smoothing * smooth_loss

        # NaN/Inf ì²´í¬
        final_loss = loss.mean()

        # ì•ˆì „ì¥ì¹˜: ë¹„ì •ìƒì ì¸ loss ê°’ ì²˜ë¦¬
        if torch.isnan(final_loss) or torch.isinf(final_loss):
            # fallbackìœ¼ë¡œ ë‹¨ìˆœí•œ cross entropy ë°˜í™˜
            ce_loss = torch.nn.functional.cross_entropy(
                pred, target, ignore_index=self.ignore_index
            )
            return (
                ce_loss
                if not (torch.isnan(ce_loss) or torch.isinf(ce_loss))
                else torch.tensor(0.0, device=pred.device, requires_grad=True)
            )

        return final_loss


class TransformerTrainer:
    def __init__(self, config, device="auto"):
        self.config = config
        self.device = (
            torch.device("cuda" if torch.cuda.is_available() else "cpu")
            if device == "auto"
            else device
        )
        self.model = None
        self.optimizer = None
        self.criterion = None
        self.scheduler = None
        # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ë¥¼ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        self.checkpoint_files = []

        # Update frequencyì™€ gradient accumulation ì„¤ì •
        self.update_freq = self.config["training"].get("update_freq", 1)
        self.accumulated_loss = 0.0
        self.accumulated_tokens = 0
        self.update_step = 0  # ì‹¤ì œ ì—…ë°ì´íŠ¸ ìŠ¤í… (gradient accumulation ê³ ë ¤)

        # WMP (Words/tokens per Minute) ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
        self.start_time = None
        self.total_tokens_processed = 0
        self.last_wmp_time = None
        self.last_wmp_tokens = 0

        # Mixed precision ì„¤ì • (ë” ì•ˆì „í•œ ì„¤ì •)
        if self.device.type == "cuda":
            self.scaler = GradScaler(
                init_scale=2**8,  # ë§¤ìš° ë‚®ì€ ì´ˆê¸° ìŠ¤ì¼€ì¼ (256)
                growth_factor=1.5,  # ë” ë³´ìˆ˜ì ì¸ ì¦ê°€ìœ¨
                backoff_factor=0.8,  # ë” ë³´ìˆ˜ì ì¸ ê°ì†Œìœ¨
                growth_interval=2000,  # ë” ì²œì²œíˆ ìŠ¤ì¼€ì¼ ì¦ê°€
            )
            self.use_amp = False
            # ìŠ¤ì¼€ì¼ë§ ë””ë²„ê¹…ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
            self.scale_overflow_count = 0
            self.last_scale_check_step = 0
        else:
            self.scaler = None
            self.use_amp = False

        print(f"Trainer initialized with device: {self.device}")
        print(f"Mixed Precision (AMP): {'Enabled' if self.use_amp else 'Disabled'}")
        print(f"Model config: {config.get('description', 'Custom config')}")

    def prepare_data(self):
        """ë°ì´í„° ì¤€ë¹„ (ì‹¤ì œ ë°ì´í„° íŒŒì¼ ì‚¬ìš©, BPE í† í¬ë‚˜ì´ì €)"""
        print("Preparing data with BPE tokenizers...")

        # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
        train_data, val_data, test_data = load_problem_data(self.config)
        train_src, train_tgt = train_data
        val_src, val_tgt = val_data

        print(f"Loaded data:")
        print(f"  - Train pairs: {len(train_src):,}")
        print(f"  - Valid pairs: {len(val_src):,}")
        print(f"  - Test pairs: {len(test_data[0]):,}")

        # ë°ì´í„° í´ë¦¬ë‹ ì ìš© (config ì„¤ì •ì— ë”°ë¼)
        data_config = self.config["data"]
        if data_config.get("apply_cleaning", True):
            print("Applying data cleaning...")
            train_src, train_tgt = clean_sentence_pairs(train_src, train_tgt)
            val_src, val_tgt = clean_sentence_pairs(val_src, val_tgt)

            print(f"After cleaning:")
            print(f"  - Train pairs: {len(train_src):,}")
            print(f"  - Valid pairs: {len(val_src):,}")

        # ë¹ˆ ë°ì´í„° í™•ì¸
        if not train_src or not train_tgt:
            print("âš ï¸  No training data found! Creating sample data for testing...")
            from .data_loader import create_data_sample_for_testing

            # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            create_data_sample_for_testing()

            # ë‹¤ì‹œ ë¡œë“œ
            train_data, val_data, test_data = load_problem_data(self.config)
            train_src, train_tgt = train_data
            val_src, val_tgt = val_data

            print(f"Using sample data:")
            print(f"  - Train pairs: {len(train_src):,}")
            print(f"  - Valid pairs: {len(val_src):,}")

        # BPE í† í¬ë‚˜ì´ì € ìƒì„±/ë¡œë“œ
        print("Creating/Loading BPE tokenizers...")
        self.src_tokenizer, self.tgt_tokenizer = create_bpe_tokenizers(self.config)

        # í† í¬ë‚˜ì´ì € ì €ì¥ (ì´ë¯¸ .model íŒŒì¼ë¡œ ì €ì¥ë¨)
        save_bpe_tokenizers(self.src_tokenizer, self.tgt_tokenizer)

        # BPE ê¸°ë°˜ í† í° ë°ì´í„° ë¡œë” ìƒì„±
        batch_tokens = self.config["training"]["batch_tokens"]
        max_length = data_config["max_length"]

        print(
            f"Creating BPE token-based data loaders with {batch_tokens} tokens per batch..."
        )

        self.train_loader = create_bpe_token_based_data_loader(
            train_src,
            train_tgt,
            self.src_tokenizer,
            self.tgt_tokenizer,
            batch_tokens=batch_tokens,
            max_length=max_length,
            shuffle=True,
        )
        self.val_loader = create_bpe_token_based_data_loader(
            val_src,
            val_tgt,
            self.src_tokenizer,
            self.tgt_tokenizer,
            batch_tokens=batch_tokens,
            max_length=max_length,
            shuffle=False,
        )

        print(f"Source vocabulary size: {self.src_tokenizer.get_vocab_size()}")
        print(f"Target vocabulary size: {self.tgt_tokenizer.get_vocab_size()}")

        # ë°°ì¹˜ ì •ë³´ ì¶œë ¥
        sample_batch = next(iter(self.train_loader))
        src_tokens = (sample_batch["src"] != 0).sum().item()
        tgt_tokens = (sample_batch["tgt_input"] != 0).sum().item()
        total_tokens = src_tokens + tgt_tokens

        print(f"Sample batch info:")
        print(f"  - Batch size (sentences): {sample_batch['src'].size(0)}")
        print(f"  - Max sequence length: {sample_batch['src'].size(1)}")
        print(f"  - Source tokens: {src_tokens}")
        print(f"  - Target tokens: {tgt_tokens}")
        print(f"  - Total tokens in batch: {total_tokens}")
        print(f"  - Target batch tokens: {batch_tokens}")

    def build_model(self):
        """ëª¨ë¸ ìƒì„±"""
        print("Building model...")

        model_config = self.config["model"]

        self.model = Transformer(
            src_vocab_size=self.src_tokenizer.get_vocab_size(),
            tgt_vocab_size=self.tgt_tokenizer.get_vocab_size(),
            d_model=model_config["d_model"],
            n_heads=model_config["h"],
            n_layers=model_config["N"],
            d_ff=model_config["d_ff"],
            max_seq_length=model_config["max_seq_length"],
        ).to(self.device)

        # Gradient Checkpointing í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
        if hasattr(self.model, "gradient_checkpointing_enable"):
            self.model.gradient_checkpointing_enable()
            print("âœ“ Gradient checkpointing enabled")
        else:
            print(
                "âš ï¸  Gradient checkpointing not available - implementing manual checkpointing"
            )

        # ë“œë¡­ì•„ì›ƒ ì„¤ì • (ëª¨ë¸ì— ë“œë¡­ì•„ì›ƒì´ ìˆë‹¤ë©´)
        for module in self.model.modules():
            if isinstance(module, nn.Dropout):
                module.p = model_config["P_drop"]

        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(
            p.numel() for p in self.model.parameters() if p.requires_grad
        )

        print(f"Total parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,}")
        print(f"Model size (FP32): {total_params * 4 / (1024**2):.2f} MB")
        print(f"Model size (FP16): {total_params * 2 / (1024**2):.2f} MB")

        # ì˜ˆìƒ VRAM ì‚¬ìš©ëŸ‰ ê³„ì‚°
        model_memory = (
            total_params * 2 / (1024**2)
            if self.use_amp
            else total_params * 4 / (1024**2)
        )  # FP16/FP32
        gradient_memory = model_memory  # ê·¸ë˜ë””ì–¸íŠ¸
        optimizer_memory = model_memory * 2  # Adam: momentum + velocity
        estimated_vram = (
            model_memory + gradient_memory + optimizer_memory
        ) * 1.3  # í™œì„±í™” + ì˜¤ë²„í—¤ë“œ

        print(
            f"Estimated VRAM usage ({'FP16' if self.use_amp else 'FP32'}): {estimated_vram:.0f} MB"
        )

        print("ï¿½ Memory Optimizations Applied:")
        print(
            f"   âœ“ Mixed Precision Training: {'Enabled' if self.use_amp else 'Disabled'}"
        )
        print(f"   âœ“ Gradient Checkpointing: Enabled")
        print(f"   âœ“ Memory-efficient Label Smoothing: Enabled")
        print(f"   âœ“ Estimated memory savings: ~40-60%")

    def setup_training(self):
        """í•™ìŠµ ì„¤ì •"""
        print("Setting up training...")

        training_config = self.config["training"]

        # ğŸš€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        print("ğŸ”§ Optimizer Memory Analysis:")
        model_params = sum(p.numel() for p in self.model.parameters())
        model_memory_mb = model_params * 2 / (1024**2)  # FP16
        adam_state_memory_mb = model_params * 2 * 4 / (1024**2)  # 2 states Ã— FP32

        print(f"   Model parameters: {model_params:,}")
        print(f"   Model memory (FP16): {model_memory_mb:.1f} MB")
        print(f"   Adam state memory: {adam_state_memory_mb:.1f} MB")
        print(f"   Total optimizer overhead: {adam_state_memory_mb:.1f} MB")

        # ì˜µí‹°ë§ˆì´ì € (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„¤ì •)
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=training_config["learning_rate"],
            betas=(0.9, 0.98),
            eps=1e-9,
            # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ì„¤ì •ë“¤
            foreach=False,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì—…ë°ì´íŠ¸
        )

        # ì†ì‹¤ í•¨ìˆ˜
        self.criterion = LabelSmoothingLoss(
            self.tgt_tokenizer.get_vocab_size(),
            smoothing=training_config["label_smoothing"],
            ignore_index=0,
        )

        # Transformer LR ìŠ¤ì¼€ì¤„ëŸ¬ (ë°°ì¹˜ í† í° ê°œìˆ˜ ê³ ë ¤)
        model_config = self.config["model"]
        batch_tokens = self.config["training"]["batch_tokens"]
        warmup_steps = training_config["warmup_steps"]

        self.scheduler = TransformerLRScheduler(
            optimizer=self.optimizer,
            d_model=model_config["d_model"],
            warmup_steps=warmup_steps,
            batch_tokens=batch_tokens,
            base_batch_tokens=25000,  # ê¸°ì¤€ ë°°ì¹˜ í† í° ìˆ˜
        )

    def evaluate(self, max_batches=20):
        """í‰ê°€ (ì œí•œëœ ë°°ì¹˜ ìˆ˜ë¡œ, Mixed Precision ì§€ì›)"""
        self.model.eval()
        total_loss = 0
        num_batches = 0

        with torch.no_grad():
            for batch_idx, batch in enumerate(self.val_loader):
                if batch_idx >= max_batches:
                    break

                src = batch["src"].to(self.device)
                tgt_input = batch["tgt_input"].to(self.device)
                tgt_output = batch["tgt_output"].to(self.device)

                # Mixed precisionìœ¼ë¡œ í‰ê°€
                if self.use_amp:
                    with autocast():
                        output = self.model(
                            src, tgt_input, src_pad_idx=0, tgt_pad_idx=0
                        )
                        loss = self.criterion(output, tgt_output)
                else:
                    output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                    loss = self.criterion(output, tgt_output)

                total_loss += loss.item()
                num_batches += 1

        return total_loss / num_batches if num_batches > 0 else 0

    def train(self, train_steps=None, save_dir="checkpoints"):
        """ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ (ìŠ¤í… ê¸°ë°˜)"""
        if train_steps is None:
            train_steps = self.config["training"]["train_steps"]

        os.makedirs(save_dir, exist_ok=True)

        # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ ë¡œë“œ ë° ì •ë¦¬
        self.load_existing_checkpoints(save_dir)

        training_config = self.config["training"]
        eval_every = training_config.get("eval_every", 500)
        save_every = training_config.get("save_every", 1000)

        train_losses = []
        val_losses = []
        steps = []
        best_val_loss = float("inf")

        # í˜„ì¬ ìŠ¤ì¼€ì¤„ëŸ¬ì˜ ìŠ¤í…ì„ í™•ì¸í•˜ì—¬ ì¬ì‹œì‘ ì§€ì  ê²°ì •
        current_step = (
            self.scheduler.step_num if hasattr(self.scheduler, "step_num") else 0
        )

        if current_step > 0:
            print(f"\nğŸ”„ Resuming training from step {current_step}")
            print(f"Target training steps: {train_steps}")
            print(f"Remaining steps: {train_steps - current_step}")
        else:
            print(f"\nğŸš€ Starting new training for {train_steps} steps")

        print(f"Evaluation every {eval_every} steps")
        print(f"Checkpoint save every {save_every} steps")
        print("=" * 60)

        start_time = time.time()
        self.start_time = start_time
        self.last_wmp_time = start_time
        self.model.train()

        # ë¬´í•œ ë°ì´í„° ë¡œë” ìƒì„± (train_stepsë§Œí¼ ë°˜ë³µ)
        def infinite_dataloader(dataloader):
            while True:
                for batch in dataloader:
                    yield batch

        data_iter = infinite_dataloader(self.train_loader)
        running_loss = 0
        log_every = 50  # 50ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥

        # Gradient accumulationì„ ìœ„í•œ ë³€ìˆ˜ë“¤
        self.accumulated_loss = 0.0
        self.accumulated_tokens = 0

        # í˜„ì¬ ìŠ¤í…ë¶€í„° ëª©í‘œ ìŠ¤í…ê¹Œì§€ í•™ìŠµ
        for step in range(current_step + 1, train_steps + 1):
            batch = next(data_iter)
            src = batch["src"].to(self.device, non_blocking=True)
            tgt_input = batch["tgt_input"].to(self.device, non_blocking=True)
            tgt_output = batch["tgt_output"].to(self.device, non_blocking=True)

            # í˜„ì¬ ë°°ì¹˜ì˜ í† í° ìˆ˜ ê³„ì‚° (íŒ¨ë”© ì œì™¸)
            current_batch_tokens = (tgt_output != 0).sum().item()
            self.total_tokens_processed += current_batch_tokens
            self.accumulated_tokens += current_batch_tokens

            # Gradient accumulation ì‹œì‘ ì‹œì—ë§Œ zero_grad
            if step % self.update_freq == 1 or self.update_freq == 1:
                self.optimizer.zero_grad(set_to_none=True)  # ë©”ëª¨ë¦¬ ì ˆì•½

            # Mixed Precision Training with ê°•í™”ëœ ì•ˆì „ì„± ì²´í¬
            if self.use_amp:
                with autocast():
                    output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                    loss = self.criterion(output, tgt_output)

                # ğŸ” Loss ì•ˆì „ì„± ì²´í¬
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âš ï¸  NaN/Inf loss detected at step {step}!")
                    print(f"   Loss value: {loss.item()}")
                    print(
                        f"   Output stats: min={output.min():.4f}, max={output.max():.4f}"
                    )
                    print(f"   Current scale: {self.scaler.get_scale()}")
                    print(f"   Skipping this batch...")
                    self.scaler.update()  # ìŠ¤ì¼€ì¼ ì¡°ì •
                    continue

                # ğŸ” ìŠ¤ì¼€ì¼ë§ ìƒíƒœ ëª¨ë‹ˆí„°ë§ (ì£¼ê¸°ì )
                if step % 500 == 0:
                    current_scale = self.scaler.get_scale()
                    print(f"ğŸ” Scale Debug at Step {step}:")
                    print(f"   Current scale: {current_scale}")
                    print(
                        f"   Scale overflows since last check: {self.scale_overflow_count}"
                    )
                    self.scale_overflow_count = 0

                    # ìŠ¤ì¼€ì¼ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ê²½ê³ 
                    if current_scale > 2**15:  # 32768
                        print(f"âš ï¸  Scale is getting high: {current_scale}")
                        print(f"   Consider reducing growth_factor or growth_interval")

                # Lossë¥¼ update_freqë¡œ ë‚˜ëˆ„ì–´ gradient accumulation ì ìš©
                scaled_loss = loss / self.update_freq

                # Scaled backward pass
                self.scaler.scale(scaled_loss).backward()

                # ğŸ” Gradient ì•ˆì „ì„± ì²´í¬ (unscale ì „ì— ìŠ¤ì¼€ì¼ëœ gradient ì²´í¬)
                scaled_grad_norm_sq = 0
                for p in self.model.parameters():
                    if p.grad is not None:
                        scaled_grad_norm_sq += (p.grad**2).sum().item()
                scaled_grad_norm = scaled_grad_norm_sq**0.5

                # ìŠ¤ì¼€ì¼ëœ gradientê°€ ë„ˆë¬´ í¬ë©´ ì¡°ê¸° ê°ì§€
                if scaled_grad_norm > 1e10:  # ë§¤ìš° í° ê°’
                    print(f"âš ï¸  Very large scaled gradient detected at step {step}!")
                    print(f"   Scaled grad norm: {scaled_grad_norm:.2e}")
                    print(f"   Current scale: {self.scaler.get_scale()}")
                    print(f"   Skipping this batch...")
                    self.scaler.update()  # ìŠ¤ì¼€ì¼ ê°ì†Œ
                    self.scale_overflow_count += 1
                    continue

                # Gradient unscaling ë° clipping
                self.scaler.unscale_(self.optimizer)

                # ğŸ” Optimizer state ì•ˆì „ì„± ì²´í¬ (ì£¼ê¸°ì )
                if step % 1000 == 0:
                    has_inf_state = False
                    inf_param_count = 0

                    for group in self.optimizer.param_groups:
                        for p in group["params"]:
                            if p.grad is None:
                                continue
                            state = self.optimizer.state[p]
                            if len(state) > 0:  # Adam state ì¡´ì¬ í™•ì¸
                                # exp_avg, exp_avg_sq ì²´í¬
                                if "exp_avg" in state and (
                                    torch.isinf(state["exp_avg"]).any()
                                    or torch.isnan(state["exp_avg"]).any()
                                ):
                                    has_inf_state = True
                                    inf_param_count += 1
                                if "exp_avg_sq" in state and (
                                    torch.isinf(state["exp_avg_sq"]).any()
                                    or torch.isnan(state["exp_avg_sq"]).any()
                                ):
                                    has_inf_state = True
                                    inf_param_count += 1

                    if has_inf_state:
                        print(
                            f"ğŸš¨ CRITICAL: Inf/NaN detected in optimizer state at step {step}!"
                        )
                        print(f"   Parameters with inf/nan states: {inf_param_count}")
                        print(f"   Current scale: {self.scaler.get_scale()}")
                        print(f"   Resetting optimizer states...")

                        # Optimizer state ë¦¬ì…‹
                        self.optimizer.state.clear()
                        # ìŠ¤ì¼€ì¼ë„ í¬ê²Œ ì¤„ì„
                        self.scaler._scale.fill_(2**8)  # 256ìœ¼ë¡œ ë¦¬ì…‹
                        continue

                grad_norm = torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), training_config["grad_clip"]
                )

                # Unscaled gradient ì²´í¬
                if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                    print(
                        f"âš ï¸  NaN/Inf unscaled gradient at step {step}! Grad norm: {grad_norm}"
                    )
                    print(f"   Current scale: {self.scaler.get_scale()}")
                    self.scaler.update()
                    self.scale_overflow_count += 1
                    continue

                # Update frequencyì— ë”°ë¥¸ ì‹¤ì œ optimizer step
                if step % self.update_freq == 0:
                    # ì •ìƒì ì¸ step
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    # Gradient accumulation ì¤‘ì¼ ë•ŒëŠ” scaler updateë§Œ
                    self.scaler.update()
            else:
                # ì¼ë°˜ FP32 í•™ìŠµ
                output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                loss = self.criterion(output, tgt_output)

                # NaN ì²´í¬
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âš ï¸  NaN/Inf loss detected at step {step}!")
                    print(f"   Loss value: {loss.item()}")
                    print(f"   Skipping this batch...")
                    continue

                # ğŸ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë””ë²„ê¹…
                # if step % 100 == 1:  # 100ìŠ¤í…ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì²´í¬
                #     torch.cuda.empty_cache()  # ìºì‹œ ì •ë¦¬
                #     print(f"ğŸ” Memory Debug at Step {step}:")
                #     print(f"   Before backward: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

                # Lossë¥¼ update_freqë¡œ ë‚˜ëˆ„ì–´ gradient accumulation ì ìš©
                scaled_loss = loss / self.update_freq
                scaled_loss.backward()

                # if step % 100 == 1:
                #     print(f"   After backward: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
                #     print(f"   Reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
                #     print(f"   Max allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")

                grad_norm = torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), training_config["grad_clip"]
                )

                if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                    print(f"âš ï¸  NaN/Inf gradient detected at step {step}!")
                    continue

                self.optimizer.step()

            # Loss accumulation (ì›ë³¸ loss ì‚¬ìš©)
            self.accumulated_loss += loss.item()

            # Update frequencyì— ë”°ë¥¸ ì‹¤ì œ optimizer/scheduler step
            if step % self.update_freq == 0:
                if (
                    not self.use_amp
                ):  # FP32ì¼ ë•Œë§Œ ì—¬ê¸°ì„œ ì‹¤í–‰ (AMPëŠ” ìœ„ì—ì„œ ì´ë¯¸ ì‹¤í–‰ë¨)
                    pass  # AMPì—ì„œëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë¨

                self.scheduler.step()
                self.update_step += 1

                # Accumulated loss í‰ê· 
                avg_accumulated_loss = self.accumulated_loss / self.update_freq
                running_loss += avg_accumulated_loss

                # Reset accumulation
                self.accumulated_loss = 0.0
                accumulated_tokens_for_update = self.accumulated_tokens
                self.accumulated_tokens = 0
            else:
                # Gradient accumulation ì¤‘ì¼ ë•ŒëŠ” scheduler step í•˜ì§€ ì•ŠìŒ
                pass

            # ì£¼ê¸°ì  ë¡œê·¸ ì¶œë ¥ (ì‹¤ì œ ì—…ë°ì´íŠ¸ê°€ ë°œìƒí•œ ìŠ¤í…ì—ì„œë§Œ)
            if step % log_every == 0 and step % self.update_freq == 0:
                num_updates = log_every // self.update_freq
                if num_updates > 0:
                    avg_loss = running_loss / num_updates
                else:
                    avg_loss = running_loss  # fallback

                current_tokens = (src != 0).sum().item() + (tgt_input != 0).sum().item()
                batch_size = src.size(0)

                # WMP (Words/tokens per Minute) ê³„ì‚°
                current_time = time.time()
                if self.last_wmp_time is not None:
                    time_elapsed = current_time - self.last_wmp_time
                    tokens_since_last = (
                        self.total_tokens_processed - self.last_wmp_tokens
                    )
                    if time_elapsed > 0:
                        wmp = tokens_since_last / (
                            time_elapsed / 60.0
                        )  # tokens per minute
                    else:
                        wmp = 0
                else:
                    wmp = 0

                # ì „ì²´ í‰ê·  WMP
                total_time_elapsed = current_time - self.start_time
                if total_time_elapsed > 0:
                    avg_wmp = self.total_tokens_processed / (total_time_elapsed / 60.0)
                else:
                    avg_wmp = 0

                # LR ìŠ¤ì¼€ì¤„ëŸ¬ ì •ë³´
                lr_info = self.scheduler.get_lr_info()
                warmup_status = "Warmup" if lr_info["is_warmup"] else "Decay"

                print(
                    f"Step {step:5d}/{train_steps} | "
                    f"Update {self.update_step:5d} | "
                    f"Loss: {avg_loss:.4f} | "
                    f"LR: {lr_info['current_lr']:.2e} ({warmup_status}) | "
                    f"Batch: {batch_size} sents, {current_tokens} tokens | "
                    f"WMP: {wmp:.0f} (avg: {avg_wmp:.0f}) | "
                    f"UF: {self.update_freq}"
                )

                train_losses.append(avg_loss)
                steps.append(step)
                running_loss = 0

                # WMP ì¶”ì  ë³€ìˆ˜ ì—…ë°ì´íŠ¸
                self.last_wmp_time = current_time
                self.last_wmp_tokens = self.total_tokens_processed

            # í‰ê°€ (ì‹¤ì œ ì—…ë°ì´íŠ¸ê°€ ë°œìƒí•œ ìŠ¤í…ì—ì„œë§Œ)
            if step % eval_every == 0 and step % self.update_freq == 0:
                print(
                    f"\n--- Evaluation at step {step} (update {self.update_step}) ---"
                )
                val_loss = self.evaluate()
                val_losses.append(val_loss)

                print(f"Val Loss: {val_loss:.4f}")

                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    torch.save(
                        {
                            "step": step,
                            "update_step": self.update_step,
                            "model_state_dict": self.model.state_dict(),
                            "optimizer_state_dict": self.optimizer.state_dict(),
                            "scheduler_state_dict": self.scheduler.state_dict(),
                            "val_loss": val_loss,
                            "config": self.config,
                        },
                        os.path.join(save_dir, "best_model.pth"),
                    )
                    print(f"âœ“ Best model saved with val loss: {val_loss:.4f}")

                self.model.train()  # í‰ê°€ í›„ ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ
                print("-" * 40)

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì‹¤ì œ ì—…ë°ì´íŠ¸ê°€ ë°œìƒí•œ ìŠ¤í…ì—ì„œë§Œ)
            if step % save_every == 0 and step % self.update_freq == 0:
                checkpoint_path = os.path.join(save_dir, f"checkpoint_step_{step}.pth")
                torch.save(
                    {
                        "step": step,
                        "update_step": self.update_step,
                        "model_state_dict": self.model.state_dict(),
                        "optimizer_state_dict": self.optimizer.state_dict(),
                        "scheduler_state_dict": self.scheduler.state_dict(),
                        "config": self.config,
                    },
                    checkpoint_path,
                )
                print(f"Checkpoint saved at step {step} (update {self.update_step})")

                # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ (ê°œìˆ˜ ì œí•œ)
                self.manage_checkpoints(checkpoint_path, save_dir)

                # ğŸ§¹ ì£¼ê¸°ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬
                if self.device.type == "cuda":
                    torch.cuda.empty_cache()
                    print(
                        f"   GPU memory cleaned: {torch.cuda.memory_allocated() / 1024**3:.2f} GB allocated"
                    )

        total_time = time.time() - start_time
        print(f"\nTraining completed in {total_time/3600:.2f} hours")
        print(f"Total steps: {train_steps}")
        print(f"Best validation loss: {best_val_loss:.4f}")

        # í•™ìŠµ ê³¡ì„  ì €ì¥
        self.save_training_curves_steps(steps, train_losses, val_losses, save_dir)

        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        results = {
            "config": self.config,
            "train_steps": train_steps,
            "steps": steps,
            "train_losses": train_losses,
            "val_losses": val_losses,
            "best_val_loss": best_val_loss,
            "total_params": sum(p.numel() for p in self.model.parameters()),
            "training_time": total_time,
        }

        with open(os.path.join(save_dir, "training_results.json"), "w") as f:
            json.dump(results, f, indent=2)

        return steps, train_losses, val_losses

    def manage_checkpoints(self, new_checkpoint_path, save_dir):
        """ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬: ìµœëŒ€ ê°œìˆ˜ ì œí•œ ë° ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ"""
        training_config = self.config["training"]
        max_checkpoints = training_config.get("max_checkpoints", 5)  # ê¸°ë³¸ê°’: 5ê°œ

        # ìƒˆ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        self.checkpoint_files.append(new_checkpoint_path)

        # ìµœëŒ€ ê°œìˆ˜ë¥¼ ì´ˆê³¼í•˜ë©´ ê°€ì¥ ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
        if len(self.checkpoint_files) > max_checkpoints:
            # ì‚­ì œí•  ì²´í¬í¬ì¸íŠ¸ (ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ)
            old_checkpoint = self.checkpoint_files.pop(0)

            # ì‹¤ì œ íŒŒì¼ ì‚­ì œ
            try:
                if os.path.exists(old_checkpoint):
                    os.remove(old_checkpoint)
                    print(
                        f"ğŸ—‘ï¸  Removed old checkpoint: {os.path.basename(old_checkpoint)}"
                    )
            except Exception as e:
                print(f"âš ï¸  Failed to remove old checkpoint {old_checkpoint}: {e}")

        # í˜„ì¬ ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ ì¶œë ¥
        print(
            f"ğŸ“ Checkpoint status: {len(self.checkpoint_files)}/{max_checkpoints} files kept"
        )

    def load_existing_checkpoints(self, save_dir):
        """ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ì„ ë°œê²¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€"""
        import glob

        # checkpoint_step_*.pth íŒ¨í„´ì˜ íŒŒì¼ë“¤ ì°¾ê¸°
        checkpoint_pattern = os.path.join(save_dir, "checkpoint_step_*.pth")
        existing_checkpoints = glob.glob(checkpoint_pattern)

        # ìŠ¤í… ë²ˆí˜¸ë¡œ ì •ë ¬ (íŒŒì¼ëª…ì—ì„œ ìŠ¤í… ë²ˆí˜¸ ì¶”ì¶œ)
        def extract_step(filename):
            import re

            match = re.search(r"checkpoint_step_(\d+)\.pth", filename)
            return int(match.group(1)) if match else 0

        existing_checkpoints.sort(key=extract_step)

        # ìµœëŒ€ ê°œìˆ˜ë§Œí¼ë§Œ ìœ ì§€
        training_config = self.config["training"]
        max_checkpoints = training_config.get("max_checkpoints", 5)

        if len(existing_checkpoints) > max_checkpoints:
            # ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ë“¤ ì‚­ì œ
            to_remove = existing_checkpoints[:-max_checkpoints]
            for checkpoint_path in to_remove:
                try:
                    os.remove(checkpoint_path)
                    print(
                        f"ğŸ—‘ï¸  Removed old checkpoint: {os.path.basename(checkpoint_path)}"
                    )
                except Exception as e:
                    print(f"âš ï¸  Failed to remove old checkpoint {checkpoint_path}: {e}")

            # ë‚¨ì€ ì²´í¬í¬ì¸íŠ¸ë“¤ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            self.checkpoint_files = existing_checkpoints[-max_checkpoints:]
        else:
            self.checkpoint_files = existing_checkpoints

        if self.checkpoint_files:
            print(f"ğŸ“ Found {len(self.checkpoint_files)} existing checkpoints")

    def load_checkpoint(self, checkpoint_path):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        print(f"Loading checkpoint from: {checkpoint_path}")

        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
        self.model.load_state_dict(checkpoint["model_state_dict"])

        # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ
        if self.optimizer and "optimizer_state_dict" in checkpoint:
            self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

        # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë¡œë“œ
        if self.scheduler and "scheduler_state_dict" in checkpoint:
            self.scheduler.load_state_dict(checkpoint["scheduler_state_dict"])

        step = checkpoint.get("step", 0)
        update_step = checkpoint.get(
            "update_step", step // self.update_freq
        )  # í˜¸í™˜ì„±ì„ ìœ„í•œ fallback
        val_loss = checkpoint.get("val_loss", float("inf"))

        # Update step ë³µì›
        self.update_step = update_step

        print(f"âœ“ Checkpoint loaded:")
        print(f"  - Step: {step}")
        print(f"  - Update step: {update_step}")
        print(f"  - Validation loss: {val_loss:.4f}")
        print(f"  - Update frequency: {self.update_freq}")

        return step, val_loss

    def save_training_curves_steps(self, steps, train_losses, val_losses, save_dir):
        """ìŠ¤í… ê¸°ë°˜ í•™ìŠµ ê³¡ì„  ì €ì¥"""
        plt.figure(figsize=(15, 5))

        plt.subplot(1, 3, 1)
        plt.plot(steps, train_losses, label="Train Loss", color="blue", linewidth=1)
        plt.xlabel("Steps")
        plt.ylabel("Loss")
        plt.title("Training Loss")
        plt.legend()
        plt.grid(True, alpha=0.3)

        # ê²€ì¦ ì†ì‹¤ì€ eval_every ê°„ê²©ìœ¼ë¡œë§Œ ìˆìœ¼ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬
        if val_losses:
            eval_every = self.config["training"].get("eval_every", 500)
            val_steps = list(
                range(eval_every, len(val_losses) * eval_every + 1, eval_every)
            )

            plt.subplot(1, 3, 2)
            plt.plot(
                val_steps,
                val_losses,
                label="Validation Loss",
                color="red",
                marker="o",
                linewidth=2,
            )
            plt.xlabel("Steps")
            plt.ylabel("Loss")
            plt.title("Validation Loss")
            plt.legend()
            plt.grid(True, alpha=0.3)

            plt.subplot(1, 3, 3)
            # ê°™ì€ êµ¬ê°„ì˜ train lossì™€ val loss ë¹„êµ
            train_at_eval = []
            for val_step in val_steps:
                # ê°€ì¥ ê°€ê¹Œìš´ train stepì˜ loss ì°¾ê¸°
                closest_idx = min(
                    range(len(steps)), key=lambda i: abs(steps[i] - val_step)
                )
                train_at_eval.append(train_losses[closest_idx])

            plt.plot(
                val_steps, train_at_eval, label="Train Loss", color="blue", linewidth=2
            )
            plt.plot(
                val_steps, val_losses, label="Validation Loss", color="red", linewidth=2
            )
            plt.xlabel("Steps")
            plt.ylabel("Loss")
            plt.title("Train vs Validation Loss")
            plt.legend()
            plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, "training_curves.png"), dpi=300)
        plt.close()
