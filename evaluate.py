"""
ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í‰ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import argparse
import torch
import torch.nn as nn
import os
import json
from tqdm import tqdm
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import glob
import re
from collections import defaultdict

from src.model import Transformer
from src.data_utils import create_tokenizer, create_token_based_data_loader, load_tokenizer
from src.trainer import LabelSmoothingLoss
from src.metrics import EvaluationMetrics, batch_decode_for_evaluation
from src.bpe_adapter import load_bpe_tokenizers, create_bpe_token_based_data_loader, save_bpe_tokenizers
from src.data_loader import load_problem_data, clean_sentence_pairs


class BeamSearchDecoder:
    """Beam Search Decoder for Transformer model"""
    
    def __init__(self, model, tgt_tokenizer, beam_size=4, alpha=0.6, max_length_offset=50):
        self.model = model
        self.tgt_tokenizer = tgt_tokenizer
        self.beam_size = beam_size
        self.alpha = alpha  # length penalty
        self.max_length_offset = max_length_offset
        self.pad_token_id = 0
        self.eos_token_id = getattr(tgt_tokenizer, 'eos_token_id', 2)
        self.bos_token_id = getattr(tgt_tokenizer, 'bos_token_id', 1)
        
    def beam_search(self, src, src_mask=None):
        """
        Beam search decoding for a single source sequence
        Args:
            src: [1, src_len] source sequence
            src_mask: [1, src_len] source mask (optional)
        Returns:
            best_sequence: [tgt_len] best decoded sequence
        """
        batch_size = src.size(0)
        assert batch_size == 1, "Beam search currently supports batch_size=1"
        
        device = src.device
        src_len = src.size(1)
        max_length = src_len + self.max_length_offset
        
        # Encode source
        with torch.no_grad():
            # Get encoder output (assuming model has separate encoder method)
            if hasattr(self.model, 'encode'):
                encoder_output = self.model.encode(src, src_mask)
            else:
                # Fallback: run full model with dummy target to get encoder states
                dummy_tgt = torch.tensor([[self.bos_token_id]], device=device)
                _ = self.model(src, dummy_tgt, src_pad_idx=self.pad_token_id, tgt_pad_idx=self.pad_token_id)
                # This is a simplified approach; ideally model should expose encoder
                encoder_output = None
        
        # Initialize beam
        beams = [(torch.tensor([self.bos_token_id], device=device), 0.0)]  # (sequence, score)
        completed_beams = []
        
        for step in range(max_length):
            if len(beams) == 0:
                break
                
            # Collect all current sequences for batch processing
            current_sequences = []
            current_scores = []
            
            for seq, score in beams:
                if seq[-1] == self.eos_token_id:
                    # Apply length penalty and add to completed beams
                    length_penalty = ((5 + len(seq)) / 6) ** self.alpha
                    final_score = score / length_penalty
                    completed_beams.append((seq, final_score))
                else:
                    current_sequences.append(seq)
                    current_scores.append(score)
            
            if not current_sequences:
                break
            
            # Prepare batch input
            max_seq_len = max(len(seq) for seq in current_sequences)
            batch_tgt = torch.full((len(current_sequences), max_seq_len), 
                                 self.pad_token_id, device=device)
            
            for i, seq in enumerate(current_sequences):
                batch_tgt[i, :len(seq)] = seq
            
            # Expand source to match batch size
            batch_src = src.expand(len(current_sequences), -1)
            
            # Get model predictions
            with torch.no_grad():
                output = self.model(batch_src, batch_tgt, 
                                  src_pad_idx=self.pad_token_id, 
                                  tgt_pad_idx=self.pad_token_id)
                
                # Get probabilities for next token (last position)
                next_token_logits = output[:, -1, :]  # [batch_size, vocab_size]
                next_token_probs = torch.log_softmax(next_token_logits, dim=-1)
            
            # Generate new beams
            new_beams = []
            
            for i, (seq, score) in enumerate(zip(current_sequences, current_scores)):
                # Get top-k next tokens
                top_probs, top_indices = torch.topk(next_token_probs[i], self.beam_size)
                
                for prob, token_id in zip(top_probs, top_indices):
                    new_seq = torch.cat([seq, token_id.unsqueeze(0)])
                    new_score = score + prob.item()
                    new_beams.append((new_seq, new_score))
            
            # Keep only top beam_size beams
            new_beams.sort(key=lambda x: x[1], reverse=True)
            beams = new_beams[:self.beam_size]
        
        # Add remaining beams to completed beams
        for seq, score in beams:
            length_penalty = ((5 + len(seq)) / 6) ** self.alpha
            final_score = score / length_penalty
            completed_beams.append((seq, final_score))
        
        # Return best sequence
        if completed_beams:
            best_seq, best_score = max(completed_beams, key=lambda x: x[1])
            return best_seq[1:]  # Remove BOS token
        else:
            # Fallback to first beam
            return beams[0][0][1:] if beams else torch.tensor([self.eos_token_id], device=device)
    
    def decode_batch(self, src_batch, src_mask_batch=None):
        """
        Decode a batch of sequences using beam search
        Args:
            src_batch: [batch_size, src_len]
            src_mask_batch: [batch_size, src_len] (optional)
        Returns:
            decoded_sequences: list of decoded sequences
        """
        batch_size = src_batch.size(0)
        decoded_sequences = []
        
        for i in range(batch_size):
            src = src_batch[i:i+1]  # [1, src_len]
            src_mask = src_mask_batch[i:i+1] if src_mask_batch is not None else None
            
            decoded_seq = self.beam_search(src, src_mask)
            decoded_sequences.append(decoded_seq)
        
        return decoded_sequences

class ModelEvaluator:
    def __init__(self, checkpoint_path, device='auto', use_averaging=True, use_beam_search=True):
        self.checkpoint_path = checkpoint_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device == 'auto' else device
        self.model = None
        self.config = None
        self.src_tokenizer = None
        self.tgt_tokenizer = None
        self.criterion = None
        self.use_averaging = use_averaging
        self.use_beam_search = use_beam_search
        self.beam_decoder = None
        
        print(f"Evaluator initialized with device: {self.device}")
        print(f"Checkpoint averaging: {'Enabled' if use_averaging else 'Disabled'}")
        print(f"Beam search: {'Enabled' if use_beam_search else 'Disabled'}")
        
    def find_recent_checkpoints(self, checkpoint_dir, max_checkpoints):
        """ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ë“¤ ì°¾ê¸°"""
        checkpoint_pattern = os.path.join(checkpoint_dir, 'checkpoint_step_*.pth')
        checkpoint_files = glob.glob(checkpoint_pattern)
        
        if not checkpoint_files:
            return []
        
        # ìŠ¤í… ë²ˆí˜¸ë¡œ ì •ë ¬
        def extract_step(filename):
            match = re.search(r'checkpoint_step_(\d+)\.pth', filename)
            return int(match.group(1)) if match else 0
        
        checkpoint_files.sort(key=extract_step, reverse=True)
        return checkpoint_files[:max_checkpoints]
    
    def load_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (averaging ì§€ì›)"""
        if self.use_averaging:
            return self.load_averaged_checkpoint()
        else:
            return self.load_single_checkpoint()
    
    def load_single_checkpoint(self):
        """ë‹¨ì¼ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        print(f"Loading single checkpoint from: {self.checkpoint_path}")
        
        if not os.path.exists(self.checkpoint_path):
            raise FileNotFoundError(f"Checkpoint not found: {self.checkpoint_path}")
        
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
        self.config = checkpoint['config']
        
        print(f"Checkpoint info:")
        print(f"  - Config: {self.config.get('description', 'Custom config')}")
        if 'step' in checkpoint:
            print(f"  - Training step: {checkpoint['step']}")
        if 'val_loss' in checkpoint:
            print(f"  - Validation loss: {checkpoint['val_loss']:.4f}")
        
        return checkpoint
    
    def load_averaged_checkpoint(self):
        """ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ í‰ê· í•˜ì—¬ ë¡œë“œ"""
        print(f"Loading and averaging multiple checkpoints...")
        
        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        if os.path.isfile(self.checkpoint_path):
            checkpoint_dir = os.path.dirname(self.checkpoint_path)
            
            # config ë¡œë“œë¥¼ ìœ„í•´ ì²« ë²ˆì§¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            first_checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
            self.config = first_checkpoint['config']
        else:
            checkpoint_dir = self.checkpoint_path
            # ë””ë ‰í† ë¦¬ì—ì„œ ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ë¡œ config ë¡œë“œ
            recent_checkpoints = self.find_recent_checkpoints(checkpoint_dir, 1)
            if not recent_checkpoints:
                raise FileNotFoundError(f"No checkpoints found in: {checkpoint_dir}")
            first_checkpoint = torch.load(recent_checkpoints[0], map_location=self.device)
            self.config = first_checkpoint['config']
        
        # configì—ì„œ max_checkpoints ì½ê¸° (ê¸°ë³¸ê°’: 5)
        max_checkpoints = self.config['training'].get('max_checkpoints', 5)
        print(f"  - Using max_checkpoints from config: {max_checkpoints}")
        
        # ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ë“¤ ì°¾ê¸°
        recent_checkpoints = self.find_recent_checkpoints(checkpoint_dir, max_checkpoints)
        
        if not recent_checkpoints:
            raise FileNotFoundError(f"No checkpoints found in: {checkpoint_dir}")
        
        print(f"  - Found {len(recent_checkpoints)} checkpoints to average:")
        for i, cp_path in enumerate(recent_checkpoints):
            cp_name = os.path.basename(cp_path)
            print(f"    {i+1}. {cp_name}")
        
        # ì²´í¬í¬ì¸íŠ¸ë“¤ ë¡œë“œ ë° í‰ê·  ê³„ì‚°
        averaged_state_dict = {}
        checkpoint_info = {'steps': [], 'val_losses': []}
        
        for i, cp_path in enumerate(recent_checkpoints):
            print(f"  - Loading checkpoint {i+1}/{len(recent_checkpoints)}: {os.path.basename(cp_path)}")
            checkpoint = torch.load(cp_path, map_location=self.device)
            
            # ì •ë³´ ìˆ˜ì§‘
            if 'step' in checkpoint:
                checkpoint_info['steps'].append(checkpoint['step'])
            if 'val_loss' in checkpoint:
                checkpoint_info['val_losses'].append(checkpoint['val_loss'])
            
            # ëª¨ë¸ ìƒíƒœ í‰ê· í™”
            model_state = checkpoint['model_state_dict']
            
            if i == 0:
                # ì²« ë²ˆì§¸ ì²´í¬í¬ì¸íŠ¸ë¡œ ì´ˆê¸°í™”
                for key, value in model_state.items():
                    averaged_state_dict[key] = value.clone().float()
            else:
                # í‰ê· ì— ì¶”ê°€
                for key, value in model_state.items():
                    if key in averaged_state_dict:
                        averaged_state_dict[key] += value.float()
        
        # í‰ê·  ê³„ì‚°
        num_checkpoints = len(recent_checkpoints)
        for key in averaged_state_dict:
            averaged_state_dict[key] /= num_checkpoints
        
        # í‰ê· í™”ëœ ì²´í¬í¬ì¸íŠ¸ ìƒì„±
        averaged_checkpoint = {
            'model_state_dict': averaged_state_dict,
            'config': self.config,
            'averaged_from': len(recent_checkpoints),
            'checkpoint_steps': checkpoint_info['steps'],
            'checkpoint_val_losses': checkpoint_info['val_losses']
        }
        
        print(f"âœ“ Averaged {num_checkpoints} checkpoints")
        if checkpoint_info['steps']:
            print(f"  - Step range: {min(checkpoint_info['steps'])} - {max(checkpoint_info['steps'])}")
        if checkpoint_info['val_losses']:
            avg_val_loss = sum(checkpoint_info['val_losses']) / len(checkpoint_info['val_losses'])
            print(f"  - Average validation loss: {avg_val_loss:.4f}")
        
        return averaged_checkpoint
    
    def load_tokenizers(self):
        """BPE í† í¬ë‚˜ì´ì € ë¡œë“œ (trainerì™€ ë™ì¼í•œ ë°©ì‹)"""
        print("Loading BPE tokenizers...")
        
        src_model_path = "tokenizers/src_bpe.model"
        tgt_model_path = "tokenizers/tgt_bpe.model"
        
        if os.path.exists(src_model_path) and os.path.exists(tgt_model_path):
            self.src_tokenizer, self.tgt_tokenizer = load_bpe_tokenizers()
            print(f"âœ“ Loaded BPE tokenizers from saved model files")
        else:
            print("âš ï¸  Saved BPE tokenizers not found. Creating new BPE tokenizers...")
            from src.bpe_adapter import create_bpe_tokenizers
            
            # ìƒˆë¡œìš´ BPE í† í¬ë‚˜ì´ì € ìƒì„± (trainerì™€ ë™ì¼í•œ ë°©ì‹)
            self.src_tokenizer, self.tgt_tokenizer = create_bpe_tokenizers(self.config)
            
            # í† í¬ë‚˜ì´ì € ì €ì¥ (trainerì™€ ë™ì¼í•œ ë°©ì‹)
            save_bpe_tokenizers(self.src_tokenizer, self.tgt_tokenizer)
            print(f"âœ“ Created and saved new BPE tokenizers")
        
        print(f"Source vocabulary size: {self.src_tokenizer.get_vocab_size()}")
        print(f"Target vocabulary size: {self.tgt_tokenizer.get_vocab_size()}")
    
    def build_model(self, checkpoint):
        """ëª¨ë¸ êµ¬ì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ (trainerì™€ ë™ì¼í•œ ë°©ì‹)"""
        print("Building and loading model...")
        
        model_config = self.config['model']
        
        self.model = Transformer(
            src_vocab_size=self.src_tokenizer.get_vocab_size(),
            tgt_vocab_size=self.tgt_tokenizer.get_vocab_size(),
            d_model=model_config['d_model'],
            n_heads=model_config['h'],
            n_layers=model_config['N'],
            d_ff=model_config['d_ff'],
            max_seq_length=model_config['max_seq_length']
        ).to(self.device)
        
        # Gradient Checkpointing í™œì„±í™” (trainerì™€ ë™ì¼)
        if hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()
            print("âœ“ Gradient checkpointing enabled")
        else:
            print("âš ï¸  Gradient checkpointing not available")
        
        # ë“œë¡­ì•„ì›ƒ ì„¤ì • (trainerì™€ ë™ì¼í•œ ë°©ì‹)
        for module in self.model.modules():
            if isinstance(module, nn.Dropout):
                module.p = model_config['P_drop']
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"âœ“ Model loaded successfully")
        print(f"Total parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,}")
        print(f"Model size (FP32): {total_params * 4 / (1024**2):.2f} MB")
        print(f"Model size (FP16): {total_params * 2 / (1024**2):.2f} MB")
        
        # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • (trainerì™€ ë™ì¼í•œ LabelSmoothingLoss)
        training_config = self.config['training']
        self.criterion = LabelSmoothingLoss(
            self.tgt_tokenizer.get_vocab_size(),
            smoothing=training_config['label_smoothing'],
            ignore_index=0
        )
        
        print(f"âœ“ Label smoothing loss initialized (smoothing={training_config['label_smoothing']})")
        
        # Beam Search Decoder ì´ˆê¸°í™”
        if self.use_beam_search:
            # Transformer paper ì„¤ì •: beam_size=4, alpha=0.6
            beam_size = 4
            alpha = 0.6
            max_length_offset = 50
            
            self.beam_decoder = BeamSearchDecoder(
                model=self.model,
                tgt_tokenizer=self.tgt_tokenizer,
                beam_size=beam_size,
                alpha=alpha,
                max_length_offset=max_length_offset
            )
            print(f"âœ“ Beam search decoder initialized (beam_size={beam_size}, alpha={alpha}, max_offset={max_length_offset})")
    
    def prepare_data(self, data_type='validation'):
        """í‰ê°€ìš© ë°ì´í„° ì¤€ë¹„ (trainerì™€ ë™ì¼í•œ ë°©ì‹)"""
        print(f"Preparing {data_type} data with BPE tokenizers...")
        
        # ì‹¤ì œ ë°ì´í„° ë¡œë“œ (trainerì™€ ë™ì¼í•œ ë°©ì‹)
        train_data, val_data, test_data = load_problem_data(self.config)
        train_src, train_tgt = train_data
        val_src, val_tgt = val_data
        
        print(f"Loaded data:")
        print(f"  - Train pairs: {len(train_src):,}")
        print(f"  - Valid pairs: {len(val_src):,}")
        print(f"  - Test pairs: {len(test_data[0]):,}")
        
        # ë°ì´í„° ì„ íƒ
        data_config = self.config['data']
        
        if data_type == 'validation':
            eval_src, eval_tgt = val_src, val_tgt
        elif data_type == 'test':
            eval_src, eval_tgt = test_data
        elif data_type == 'train':
            eval_src, eval_tgt = train_src, train_tgt
        else:
            raise ValueError(f"Unsupported data_type: {data_type}. Use 'train', 'validation', or 'test'")
        
        # ë°ì´í„° í´ë¦¬ë‹ ì ìš© (trainerì™€ ë™ì¼í•œ ë°©ì‹)
        if data_config.get('apply_cleaning', True):
            print("Applying data cleaning...")
            eval_src, eval_tgt = clean_sentence_pairs(eval_src, eval_tgt)
            
            print(f"After cleaning:")
            print(f"  - {data_type.title()} pairs: {len(eval_src):,}")
        
        # ë¹ˆ ë°ì´í„° í™•ì¸ (trainerì™€ ë™ì¼í•œ ë°©ì‹)
        if not eval_src or not eval_tgt:
            print("âš ï¸  No evaluation data found! Creating sample data for testing...")
            from src.data_loader import create_data_sample_for_testing
            
            # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            create_data_sample_for_testing()
            
            # ë‹¤ì‹œ ë¡œë“œ
            train_data, val_data, test_data = load_problem_data(self.config)
            train_src, train_tgt = train_data
            val_src, val_tgt = val_data
            
            if data_type == 'validation':
                eval_src, eval_tgt = val_src, val_tgt
            elif data_type == 'test':
                eval_src, eval_tgt = test_data
            else:
                eval_src, eval_tgt = train_src, train_tgt
            
            print(f"Using sample data:")
            print(f"  - {data_type.title()} pairs: {len(eval_src):,}")
        
        # BPE ê¸°ë°˜ í† í° ë°ì´í„° ë¡œë” ìƒì„± (trainerì™€ ë™ì¼í•œ ë°©ì‹)
        batch_tokens = self.config['training']['batch_tokens']
        max_length = data_config['max_length']
        
        print(f"Creating BPE token-based data loader with {batch_tokens} tokens per batch...")
        
        self.eval_loader = create_bpe_token_based_data_loader(
            eval_src, eval_tgt, self.src_tokenizer, self.tgt_tokenizer,
            batch_tokens=batch_tokens, max_length=max_length, shuffle=False
        )
        
        # ë°°ì¹˜ ì •ë³´ ì¶œë ¥ (trainerì™€ ë™ì¼í•œ ë°©ì‹)
        sample_batch = next(iter(self.eval_loader))
        src_tokens = (sample_batch['src'] != 0).sum().item()
        tgt_tokens = (sample_batch['tgt_input'] != 0).sum().item()
        total_tokens = src_tokens + tgt_tokens
        
        print(f"Sample batch info:")
        print(f"  - Batch size (sentences): {sample_batch['src'].size(0)}")
        print(f"  - Max sequence length: {sample_batch['src'].size(1)}")
        print(f"  - Source tokens: {src_tokens}")
        print(f"  - Target tokens: {tgt_tokens}")
        print(f"  - Total tokens in batch: {total_tokens}")
        print(f"  - Target batch tokens: {batch_tokens}")
    
    def evaluate_full(self, max_batches=None):
        """ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ìƒì„¸ í‰ê°€ (trainerì˜ evaluateì™€ í˜¸í™˜)"""
        print("\nStarting full evaluation with BLEU and Perplexity metrics...")
        
        self.model.eval()
        metrics = EvaluationMetrics()
        
        with torch.no_grad():
            progress_bar = tqdm(self.eval_loader, desc="Evaluating")
            
            for batch_idx, batch in enumerate(progress_bar):
                if max_batches and batch_idx >= max_batches:
                    break
                
                # ë°ì´í„° ì´ë™ (trainerì™€ ë™ì¼í•œ ë°©ì‹)
                src = batch['src'].to(self.device, non_blocking=True)
                tgt_input = batch['tgt_input'].to(self.device, non_blocking=True)
                tgt_output = batch['tgt_output'].to(self.device, non_blocking=True)
                
                # ëª¨ë¸ ì˜ˆì¸¡ (trainerì™€ ë™ì¼í•œ ë°©ì‹)
                output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                loss = self.criterion(output, tgt_output)
                predictions = torch.argmax(output, dim=-1)
                
                # NaN/Inf ì²´í¬ (trainerì™€ ë™ì¼í•œ ì•ˆì „ì¥ì¹˜)
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âš ï¸  NaN/Inf loss detected in batch {batch_idx}!")
                    print(f"   Loss value: {loss.item()}")
                    print(f"   Output stats: min={output.min():.4f}, max={output.max():.4f}")
                    continue
                
                # ì†ì‹¤ ì—…ë°ì´íŠ¸
                batch_tokens = (tgt_output != 0).sum().item()
                metrics.update_loss(loss.item(), batch_tokens)
                
                # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©í•˜ì—¬ BLEU ìŠ¤ì½”ì–´ ê³„ì‚°
                src_texts, tgt_texts, pred_texts = batch_decode_for_evaluation(
                    src, tgt_output, predictions,
                    self.src_tokenizer, self.tgt_tokenizer, pad_token_id=0
                )
                
                # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
                valid_pairs = [(pred, tgt) for pred, tgt in zip(pred_texts, tgt_texts) 
                              if pred.strip() and tgt.strip()]
                
                if valid_pairs:
                    valid_preds, valid_tgts = zip(*valid_pairs)
                    metrics.add_predictions(list(valid_preds), list(valid_tgts))
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                current_summary = metrics.get_summary()
                if current_summary:
                    progress_bar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'avg_loss': f'{current_summary["average_loss"]:.4f}',
                        'ppl': f'{current_summary["perplexity"]:.1f}',
                        'samples': len(metrics.predictions)
                    })
        
        # ìµœì¢… ê²°ê³¼ ê³„ì‚° ë° ì¶œë ¥
        results = metrics.get_summary()
        metrics.print_summary()
        
        return results
    
    def evaluate_with_beam_search(self, max_batches=None):
        """Beam Searchë¥¼ ì‚¬ìš©í•œ í‰ê°€ (Transformer paper ë°©ì‹)"""
        if not self.use_beam_search or self.beam_decoder is None:
            print("âš ï¸  Beam search is not enabled. Using greedy decoding instead.")
            return self.evaluate_full(max_batches)
        
        print("\nStarting evaluation with Beam Search (beam_size=4, alpha=0.6)...")
        
        self.model.eval()
        metrics = EvaluationMetrics()
        
        with torch.no_grad():
            progress_bar = tqdm(self.eval_loader, desc="Beam Search Evaluation")
            
            for batch_idx, batch in enumerate(progress_bar):
                if max_batches and batch_idx >= max_batches:
                    break
                
                # ë°ì´í„° ì´ë™
                src = batch['src'].to(self.device, non_blocking=True)
                tgt_output = batch['tgt_output'].to(self.device, non_blocking=True)
                
                # Beam Search ë””ì½”ë”© (ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬)
                batch_size = src.size(0)
                beam_predictions = []
                
                for i in range(batch_size):
                    src_seq = src[i:i+1]  # [1, src_len]
                    
                    # Beam searchë¡œ ë””ì½”ë”©
                    decoded_seq = self.beam_decoder.beam_search(src_seq)
                    beam_predictions.append(decoded_seq)
                
                # ë°°ì¹˜ í¬ê¸°ì— ë§ê²Œ íŒ¨ë”©
                if beam_predictions:
                    max_pred_len = max(len(pred) for pred in beam_predictions)
                    padded_predictions = torch.full((batch_size, max_pred_len), 
                                                  self.beam_decoder.pad_token_id, 
                                                  device=self.device)
                    
                    for i, pred in enumerate(beam_predictions):
                        padded_predictions[i, :len(pred)] = pred
                    
                    predictions = padded_predictions
                else:
                    continue
                
                # Teacher forcingìœ¼ë¡œ loss ê³„ì‚° (beam searchì™€ ë³„ê°œ)
                tgt_input = batch['tgt_input'].to(self.device, non_blocking=True)
                output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                loss = self.criterion(output, tgt_output)
                
                # NaN/Inf ì²´í¬
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âš ï¸  NaN/Inf loss detected in batch {batch_idx}!")
                    continue
                
                # ì†ì‹¤ ì—…ë°ì´íŠ¸
                batch_tokens = (tgt_output != 0).sum().item()
                metrics.update_loss(loss.item(), batch_tokens)
                
                # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©í•˜ì—¬ BLEU ìŠ¤ì½”ì–´ ê³„ì‚°
                src_texts, tgt_texts, pred_texts = batch_decode_for_evaluation(
                    src, tgt_output, predictions,
                    self.src_tokenizer, self.tgt_tokenizer, pad_token_id=0
                )
                
                # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
                valid_pairs = [(pred, tgt) for pred, tgt in zip(pred_texts, tgt_texts) 
                              if pred.strip() and tgt.strip()]
                
                if valid_pairs:
                    valid_preds, valid_tgts = zip(*valid_pairs)
                    metrics.add_predictions(list(valid_preds), list(valid_tgts))
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                current_summary = metrics.get_summary()
                if current_summary:
                    progress_bar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'avg_loss': f'{current_summary["average_loss"]:.4f}',
                        'ppl': f'{current_summary["perplexity"]:.1f}',
                        'bleu': f'{current_summary.get("bleu", 0):.2f}',
                        'samples': len(metrics.predictions)
                    })
        
        # ìµœì¢… ê²°ê³¼ ê³„ì‚° ë° ì¶œë ¥
        results = metrics.get_summary()
        print(f"\nğŸ¯ Beam Search Evaluation Results:")
        print(f"   - BLEU Score: {results.get('bleu', 0):.2f}")
        print(f"   - Perplexity: {results.get('perplexity', 0):.2f}")
        print(f"   - Average Loss: {results.get('average_loss', 0):.4f}")
        metrics.print_summary()
        
        return results
    
    def evaluate_samples(self, num_samples=5):
        """ëª‡ ê°œ ìƒ˜í”Œì— ëŒ€í•œ ìƒì„¸ ë¶„ì„ (ê°œë³„ BLEU ìŠ¤ì½”ì–´ í¬í•¨)"""
        print(f"\nEvaluating {num_samples} sample translations...")
        
        self.model.eval()
        samples_evaluated = 0
        sample_metrics = EvaluationMetrics()
        
        with torch.no_grad():
            for batch in self.eval_loader:
                src = batch['src'].to(self.device)
                tgt_input = batch['tgt_input'].to(self.device)
                tgt_output = batch['tgt_output'].to(self.device)
                
                output = self.model(src, tgt_input, src_pad_idx=0, tgt_pad_idx=0)
                predictions = torch.argmax(output, dim=-1)
                
                # ë°°ì¹˜ ì „ì²´ë¥¼ ë””ì½”ë”©
                src_texts, tgt_texts, pred_texts = batch_decode_for_evaluation(
                    src, tgt_output, predictions,
                    self.src_tokenizer, self.tgt_tokenizer, pad_token_id=0
                )
                
                batch_size = src.size(0)
                for i in range(min(batch_size, num_samples - samples_evaluated)):
                    print(f"\n--- Sample {samples_evaluated + 1} ---")
                    print(f"Source: {src_texts[i]}")
                    print(f"Target: {tgt_texts[i]}")
                    print(f"Prediction: {pred_texts[i]}")
                    
                    # ê°œë³„ ìƒ˜í”Œ BLEU ìŠ¤ì½”ì–´
                    if pred_texts[i].strip() and tgt_texts[i].strip():
                        from sacrebleu import BLEU
                        bleu_scorer = BLEU()
                        sample_bleu = bleu_scorer.sentence_score(pred_texts[i], [tgt_texts[i]])
                        print(f"Sample BLEU: {sample_bleu.score:.2f}")
                    
                    # í† í° ë ˆë²¨ ì •í™•ë„
                    tgt_seq = tgt_output[i]
                    pred_seq = predictions[i]
                    mask = (tgt_seq != 0)
                    if mask.sum() > 0:
                        accuracy = (tgt_seq[mask] == pred_seq[mask]).float().mean().item()
                        print(f"Token Accuracy: {accuracy:.4f}")
                    
                    # ì „ì²´ ë©”íŠ¸ë¦­ì— ì¶”ê°€
                    if pred_texts[i].strip() and tgt_texts[i].strip():
                        sample_metrics.add_predictions([pred_texts[i]], [tgt_texts[i]])
                    
                    samples_evaluated += 1
                
                if samples_evaluated >= num_samples:
                    break
        
        print(f"\nSample evaluation completed ({samples_evaluated} samples)")
        
        # ìƒ˜í”Œë“¤ì˜ ì „ì²´ BLEU ìŠ¤ì½”ì–´
        if len(sample_metrics.predictions) > 0:
            sample_bleu_scores = sample_metrics.compute_bleu()
            print(f"\nOverall Sample BLEU Scores:")
            print(f"BLEU: {sample_bleu_scores['bleu']:.2f}")
            print(f"BLEU-1: {sample_bleu_scores['bleu_1']:.2f}")
            print(f"BLEU-2: {sample_bleu_scores['bleu_2']:.2f}")
            print(f"BLEU-3: {sample_bleu_scores['bleu_3']:.2f}")
            print(f"BLEU-4: {sample_bleu_scores['bleu_4']:.2f}")
    
    def save_results(self, results, output_dir):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)
        
        # JSON ê²°ê³¼ ì €ì¥
        results_with_metadata = {
            'checkpoint_path': self.checkpoint_path,
            'evaluation_time': datetime.now().isoformat(),
            'config': self.config,
            'device': str(self.device),
            'results': results
        }
        
        results_file = os.path.join(output_dir, 'evaluation_results.json')
        with open(results_file, 'w', encoding='utf-8') as f:
            # batch_lossesëŠ” ë„ˆë¬´ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬
            save_results = results_with_metadata.copy()
            batch_losses = save_results['results'].pop('batch_losses', [])
            json.dump(save_results, f, indent=2, ensure_ascii=False)
        
        # í‰ê°€ ê²°ê³¼ ì‹œê°í™”
        batch_losses = results.get('batch_losses', [])
        if batch_losses:
            plt.figure(figsize=(15, 5))
            
            # ë°°ì¹˜ë³„ ì†ì‹¤
            plt.subplot(1, 3, 1)
            plt.plot(batch_losses, alpha=0.7)
            plt.axhline(y=results['average_loss'], color='r', linestyle='--', 
                       label=f'Average: {results["average_loss"]:.4f}')
            plt.xlabel('Batch')
            plt.ylabel('Loss')
            plt.title('Loss per Batch')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ì†ì‹¤ ë¶„í¬
            plt.subplot(1, 3, 2)
            plt.hist(batch_losses, bins=30, alpha=0.7, edgecolor='black')
            plt.axvline(x=results['average_loss'], color='r', linestyle='--', 
                       label=f'Average: {results["average_loss"]:.4f}')
            plt.xlabel('Loss')
            plt.ylabel('Frequency')
            plt.title('Loss Distribution')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ë©”íŠ¸ë¦­ ìš”ì•½
            plt.subplot(1, 3, 3)
            metrics_names = ['Perplexity', 'BLEU', 'BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4']
            metrics_values = [
                results.get('perplexity', 0),
                results.get('bleu', 0),
                results.get('bleu_1', 0),
                results.get('bleu_2', 0),
                results.get('bleu_3', 0),
                results.get('bleu_4', 0)
            ]
            
            # PerplexityëŠ” ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥´ë¯€ë¡œ ì •ê·œí™”
            normalized_values = metrics_values.copy()
            if normalized_values[0] > 0:  # Perplexity
                normalized_values[0] = min(normalized_values[0] / 10, 100)  # ìŠ¤ì¼€ì¼ ì¡°ì •
            
            bars = plt.bar(metrics_names, normalized_values, alpha=0.7)
            plt.title('Evaluation Metrics')
            plt.ylabel('Score')
            plt.xticks(rotation=45)
            
            # ê°’ í‘œì‹œ
            for bar, value in zip(bars, metrics_values):
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{value:.2f}', ha='center', va='bottom', fontsize=8)
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'evaluation_analysis.png'), dpi=300)
            plt.close()
        
        print(f"Results saved to: {output_dir}")
        print(f"  - evaluation_results.json")
        print(f"  - loss_analysis.png")

def main():
    parser = argparse.ArgumentParser(description='ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ í‰ê°€ (Transformer paper ë°©ì‹)')
    parser.add_argument('checkpoint', type=str, help='ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--data_type', type=str, default='validation', 
                       choices=['validation', 'train', 'test'], help='í‰ê°€í•  ë°ì´í„° íƒ€ì…')
    parser.add_argument('--max_batches', type=int, default=None, 
                       help='ìµœëŒ€ í‰ê°€ ë°°ì¹˜ ìˆ˜ (Noneì´ë©´ ì „ì²´)')
    parser.add_argument('--num_samples', type=int, default=5, 
                       help='ìƒì„¸ ë¶„ì„í•  ìƒ˜í”Œ ìˆ˜')
    parser.add_argument('--output_dir', type=str, default=None, 
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--no_samples', action='store_true', 
                       help='ìƒ˜í”Œ ë¶„ì„ ìƒëµ')
    parser.add_argument('--device', type=str, default='auto',
                       help='ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (auto/cuda/cpu)')
    parser.add_argument('--no_averaging', action='store_true',
                       help='ì²´í¬í¬ì¸íŠ¸ í‰ê· í™” ë¹„í™œì„±í™”')
    parser.add_argument('--no_beam_search', action='store_true',
                       help='Beam search ë¹„í™œì„±í™” (greedy decoding ì‚¬ìš©)')
    parser.add_argument('--beam_size', type=int, default=4,
                       help='Beam search beam size (ê¸°ë³¸ê°’: 4)')
    parser.add_argument('--length_penalty', type=float, default=0.6,
                       help='Length penalty alpha (ê¸°ë³¸ê°’: 0.6)')
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir is None:
        checkpoint_name = os.path.basename(args.checkpoint).replace('.pth', '')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.output_dir = f"evaluation_{checkpoint_name}_{timestamp}"
    
    print("Transformer ëª¨ë¸ í‰ê°€ ì‹œì‘ (Transformer Paper ë°©ì‹)")
    print("=" * 70)
    print(f"ì²´í¬í¬ì¸íŠ¸: {args.checkpoint}")
    print(f"ë°ì´í„° íƒ€ì…: {args.data_type}")
    print(f"ìµœëŒ€ ë°°ì¹˜: {args.max_batches or 'All'}")
    print(f"ë””ë°”ì´ìŠ¤: {args.device}")
    print(f"ì²´í¬í¬ì¸íŠ¸ í‰ê· í™”: {'Disabled' if args.no_averaging else 'Enabled'}")
    print(f"Beam Search: {'Disabled' if args.no_beam_search else f'Enabled (size={args.beam_size}, Î±={args.length_penalty})'}")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
    print("=" * 70)
    
    # í‰ê°€ì ìƒì„±
    evaluator = ModelEvaluator(
        args.checkpoint, 
        device=args.device,
        use_averaging=not args.no_averaging,
        use_beam_search=not args.no_beam_search
    )
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = evaluator.load_checkpoint()
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    evaluator.load_tokenizers()
    
    # ëª¨ë¸ êµ¬ì„±
    evaluator.build_model(checkpoint)
    
    # ë°ì´í„° ì¤€ë¹„
    evaluator.prepare_data(args.data_type)
    
    # Beam search íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (ì‚¬ìš©ì ì„¤ì •ì´ ìˆëŠ” ê²½ìš°)
    if evaluator.use_beam_search and evaluator.beam_decoder:
        evaluator.beam_decoder.beam_size = args.beam_size
        evaluator.beam_decoder.alpha = args.length_penalty
        print(f"âœ“ Beam search parameters updated: beam_size={args.beam_size}, alpha={args.length_penalty}")
    
    # ì „ì²´ í‰ê°€ (Beam search ë˜ëŠ” ì¼ë°˜ í‰ê°€)
    if evaluator.use_beam_search:
        results = evaluator.evaluate_with_beam_search(args.max_batches)
    else:
        results = evaluator.evaluate_full(args.max_batches)
    
    # ìƒ˜í”Œ ë¶„ì„
    if not args.no_samples:
        evaluator.evaluate_samples(args.num_samples)
    
    # ê²°ê³¼ ì €ì¥
    evaluator.save_results(results, args.output_dir)
    
    print(f"\nğŸ‰ í‰ê°€ ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"   - BLEU Score: {results.get('bleu', 0):.2f}")
    print(f"   - Perplexity: {results.get('perplexity', 0):.2f}")
    print(f"   - í‰ê°€ ë°©ì‹: {'Beam Search' if evaluator.use_beam_search else 'Greedy'}")
    print(f"   - ì²´í¬í¬ì¸íŠ¸ í‰ê· í™”: {'Yes' if evaluator.use_averaging else 'No'}")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {args.output_dir}")

if __name__ == "__main__":
    main()
