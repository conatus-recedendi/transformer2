"""
Transformer ë²ˆì—­ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
JSON config íŒŒì¼ì„ í†µí•œ ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ ë³€í˜•ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import os
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
import time
from datetime import datetime
import glob

from src.trainer import TransformerTrainer


def load_config(config_path):
    """JSON config íŒŒì¼ ë¡œë“œ"""
    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)
    return config


def get_available_configs():
    """ì‚¬ìš© ê°€ëŠ¥í•œ config íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
    config_files = glob.glob("configs/*.json")
    configs = {}
    for config_file in config_files:
        config_name = os.path.basename(config_file).replace(".json", "")
        try:
            config = load_config(config_file)
            configs[config_name] = {
                "path": config_file,
                "description": config.get("description", "No description"),
                "config": config,
            }
        except Exception as e:
            print(f"Warning: Could not load config {config_file}: {e}")
    return configs


def merge_config_with_args(config, args):
    """ì»¤ë§¨ë“œë¼ì¸ ì¸ìë¡œ config ë®ì–´ì“°ê¸°"""
    if args.train_steps is not None:
        config["training"]["train_steps"] = args.train_steps
    if args.batch_tokens is not None:
        config["training"]["batch_tokens"] = args.batch_tokens
    if args.learning_rate is not None:
        config["training"]["learning_rate"] = args.learning_rate
    if args.max_length is not None:
        config["data"]["max_length"] = args.max_length
        config["model"]["max_seq_length"] = args.max_length
    if args.vocab_size is not None:
        config["data"]["vocab_size"] = args.vocab_size
    if args.data_multiplier is not None:
        config["data"]["data_multiplier"] = args.data_multiplier
    if args.warmup_steps is not None:
        config["training"]["warmup_steps"] = args.warmup_steps
    if args.update_freq is not None:
        config["training"]["update_freq"] = args.update_freq

    return config


def find_latest_checkpoint(checkpoint_dir):
    """ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°"""
    if not os.path.exists(checkpoint_dir):
        return None, None

    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, "checkpoint_step_*.pth"))

    if not checkpoint_files:
        return None, None

    # ìŠ¤í… ë²ˆí˜¸ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
    def extract_step(filepath):
        filename = os.path.basename(filepath)
        try:
            step = int(filename.replace("checkpoint_step_", "").replace(".pth", ""))
            return step
        except ValueError:
            return 0

    checkpoint_files.sort(key=extract_step, reverse=True)
    latest_checkpoint = checkpoint_files[0]
    latest_step = extract_step(latest_checkpoint)

    return latest_checkpoint, latest_step


def load_and_resume_training(trainer, checkpoint_path, target_step=None):
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì¬ê°œ"""
    print(f"Loading checkpoint from: {checkpoint_path}")

    step, val_loss = trainer.load_checkpoint(checkpoint_path)

    print(f"âœ“ Resumed from step {step}")
    if val_loss != float("inf"):
        print(f"âœ“ Previous validation loss: {val_loss:.4f}")

    # íŠ¹ì • ìŠ¤í…ë¶€í„° ì‹œì‘í•˜ë ¤ëŠ” ê²½ìš°
    if target_step is not None and target_step != step:
        print(f"âš ï¸  Requested step {target_step} but checkpoint is at step {step}")
        print(f"   Continuing from checkpoint step {step}")

    return step


# TransformerTrainerì™€ LabelSmoothingLossëŠ” src.trainerì—ì„œ importë¨


def main():
    parser = argparse.ArgumentParser(
        description="Transformer ë²ˆì—­ ëª¨ë¸ í•™ìŠµ (JSON Config ì§€ì›)"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="base",
        help="Config íŒŒì¼ ì´ë¦„ (configs/ ë””ë ‰í† ë¦¬ì˜ .json íŒŒì¼)",
    )
    parser.add_argument(
        "--config_path", type=str, default=None, help="Config íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ"
    )
    parser.add_argument(
        "--train_steps",
        type=int,
        default=None,
        help="í•™ìŠµ ìŠ¤í… ìˆ˜ (config íŒŒì¼ ë®ì–´ì“°ê¸°)",
    )
    parser.add_argument(
        "--batch_tokens",
        type=int,
        default=None,
        help="ë°°ì¹˜ í† í° ìˆ˜ (config íŒŒì¼ ë®ì–´ì“°ê¸°)",
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=None,
        help="í•™ìŠµë¥  (config íŒŒì¼ ë®ì–´ì“°ê¸°)",
    )
    parser.add_argument(
        "--max_length",
        type=int,
        default=None,
        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (config íŒŒì¼ ë®ì–´ì“°ê¸°)",
    )
    parser.add_argument(
        "--vocab_size", type=int, default=None, help="ì–´íœ˜ í¬ê¸° (config íŒŒì¼ ë®ì–´ì“°ê¸°)"
    )
    parser.add_argument(
        "--data_multiplier",
        type=int,
        default=None,
        help="ë°ì´í„° í™•ì¥ ë°°ìˆ˜ (config íŒŒì¼ ë®ì–´ì“°ê¸°)",
    )
    parser.add_argument(
        "--warmup_steps",
        type=int,
        default=None,
        help="ì›Œë°ì—… ìŠ¤í… (config íŒŒì¼ ë®ì–´ì“°ê¸°)",
    )
    parser.add_argument(
        "--update_freq",
        type=int,
        default=None,
        help="ì—…ë°ì´íŠ¸ ë¹ˆë„ (gradient accumulation)",
    )
    parser.add_argument("--save_dir", type=str, default=None, help="ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument(
        "--list_configs", action="store_true", help="ì‚¬ìš© ê°€ëŠ¥í•œ ì„¤ì • ëª©ë¡ ì¶œë ¥"
    )
    parser.add_argument(
        "--create_config",
        type=str,
        default=None,
        help="ìƒˆë¡œìš´ config íŒŒì¼ ìƒì„± (íŒŒì¼ëª…)",
    )
    parser.add_argument(
        "--evaluate", type=str, default=None, help="ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ (í‰ê°€ ëª¨ë“œ)"
    )
    parser.add_argument(
        "--eval_data_type",
        type=str,
        default="validation",
        choices=["validation", "train"],
        help="í‰ê°€í•  ë°ì´í„° íƒ€ì…",
    )
    parser.add_argument(
        "--eval_max_batches", type=int, default=None, help="í‰ê°€ ì‹œ ìµœëŒ€ ë°°ì¹˜ ìˆ˜"
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        default=None,
        help="ì²´í¬í¬ì¸íŠ¸ í´ë”ì—ì„œ ì¬ì‹œì‘ (í´ë” ê²½ë¡œ)",
    )
    parser.add_argument(
        "--resume_from_step",
        type=int,
        default=None,
        help="íŠ¹ì • ìŠ¤í…ë¶€í„° ì¬ì‹œì‘ (ì„ íƒì )",
    )

    args = parser.parse_args()

    # config íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists("configs"):
        os.makedirs("configs")
        print("configs/ ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ì‚¬ìš© ê°€ëŠ¥í•œ config ëª©ë¡ ì¶œë ¥
    if args.list_configs:
        configs = get_available_configs()
        if not configs:
            print("ì‚¬ìš© ê°€ëŠ¥í•œ config íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print(
                "configs/ ë””ë ‰í† ë¦¬ì— .json íŒŒì¼ì„ ìƒì„±í•˜ê±°ë‚˜ --create_config ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”."
            )
            return

        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„¤ì •:")
        print("=" * 80)
        for name, info in configs.items():
            config = info["config"]
            print(f"{name:15s}: {info['description']}")
            model_cfg = config["model"]
            training_cfg = config["training"]
            data_cfg = config["data"]
            print(
                f"{'':15s}  Model: N={model_cfg['N']}, d_model={model_cfg['d_model']}, "
                f"d_ff={model_cfg['d_ff']}, h={model_cfg['h']}"
            )
            print(
                f"{'':15s}  Training: train_steps={training_cfg['train_steps']}, "
                f"batch_tokens={training_cfg['batch_tokens']}, lr={training_cfg['learning_rate']}"
            )
            print(
                f"{'':15s}  Data: vocab_size={data_cfg['vocab_size']}, "
                f"max_length={data_cfg['max_length']}"
            )
            print()
        return

    # ìƒˆë¡œìš´ config íŒŒì¼ ìƒì„±
    if args.create_config:
        create_sample_config(args.create_config)
        return

    # í‰ê°€ ëª¨ë“œ
    if args.evaluate:
        from evaluate import ModelEvaluator

        print("Transformer ëª¨ë¸ í‰ê°€ ëª¨ë“œ")
        print("=" * 60)
        print(f"ì²´í¬í¬ì¸íŠ¸: {args.evaluate}")
        print(f"ë°ì´í„° íƒ€ì…: {args.eval_data_type}")
        print("=" * 60)

        # í‰ê°€ì ìƒì„±
        evaluator = ModelEvaluator(args.evaluate)

        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint = evaluator.load_checkpoint()

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        evaluator.load_tokenizers()

        # ëª¨ë¸ êµ¬ì„±
        evaluator.build_model(checkpoint)

        # ë°ì´í„° ì¤€ë¹„
        evaluator.prepare_data(args.eval_data_type)

        # í‰ê°€ ì‹¤í–‰
        results = evaluator.evaluate_full(args.eval_max_batches)

        # ëª‡ ê°œ ìƒ˜í”Œ ë¶„ì„
        evaluator.evaluate_samples(num_samples=3)

        # ê²°ê³¼ ì €ì¥
        checkpoint_name = os.path.basename(args.evaluate).replace(".pth", "")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"evaluation_{checkpoint_name}_{timestamp}"
        evaluator.save_results(results, output_dir)

        print(f"\ní‰ê°€ ì™„ë£Œ! ê²°ê³¼ëŠ” {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    # Config íŒŒì¼ ë¡œë“œ
    if args.config_path:
        config_path = args.config_path
        config_name = os.path.basename(config_path).replace(".json", "")
    else:
        config_name = args.config
        config_path = f"configs/{config_name}.json"

    if not os.path.exists(config_path):
        print(f"Error: Config íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ config ëª©ë¡ì„ ë³´ë ¤ë©´ --list_configs ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        return

    try:
        config = load_config(config_path)
        print(f"Config íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {config_path}")
    except Exception as e:
        print(f"Error: Config íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # ì»¤ë§¨ë“œë¼ì¸ ì¸ìë¡œ config ë®ì–´ì“°ê¸°
    config = merge_config_with_args(config, args)

    # ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.save_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.save_dir = f"checkpoints_{config_name}_{timestamp}"

    print("Transformer ë²ˆì—­ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 80)
    print(f"Config: {config_name} - {config.get('description', 'No description')}")
    print(f"í•™ìŠµ ìŠ¤í…: {config['training']['train_steps']}")
    print(f"ë°°ì¹˜ í† í° ìˆ˜: {config['training']['batch_tokens']}")
    print(f"í•™ìŠµë¥ : {config['training']['learning_rate']}")
    print(f"ëª¨ë¸ ì°¨ì›: {config['model']['d_model']}, ë ˆì´ì–´: {config['model']['N']}")
    print(f"ì €ì¥ ë””ë ‰í† ë¦¬: {args.save_dir}")
    print("=" * 80)

    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = TransformerTrainer(config)

    # ë°ì´í„° ì¤€ë¹„
    trainer.prepare_data()

    # ëª¨ë¸ êµ¬ì„±
    trainer.build_model()

    # í•™ìŠµ ì„¤ì •
    trainer.setup_training()

    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘ ì—¬ë¶€ í™•ì¸
    start_step = 0
    if args.checkpoint:
        print(f"\nğŸ”„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘ ëª¨ë“œ")
        print(f"ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬: {args.checkpoint}")

        # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        latest_checkpoint, latest_step = find_latest_checkpoint(args.checkpoint)

        if latest_checkpoint:
            print(
                f"âœ“ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {os.path.basename(latest_checkpoint)} (step {latest_step})"
            )

            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ë° ì¬ì‹œì‘
            start_step = load_and_resume_training(
                trainer, latest_checkpoint, args.resume_from_step
            )

            # ì €ì¥ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
            args.save_dir = args.checkpoint
            print(f"âœ“ ì €ì¥ ë””ë ‰í† ë¦¬: {args.save_dir} (ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©)")

        else:
            print(
                f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ '{args.checkpoint}'ì—ì„œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
            print(f"   ìƒˆë¡œìš´ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")

            # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
            os.makedirs(args.checkpoint, exist_ok=True)
            args.save_dir = args.checkpoint

    # ë‚¨ì€ í•™ìŠµ ìŠ¤í… ê³„ì‚°
    total_steps = config["training"]["train_steps"]
    remaining_steps = max(0, total_steps - start_step)

    if remaining_steps > 0:
        print(f"\nğŸ“ˆ í•™ìŠµ ì§„í–‰ ì •ë³´:")
        print(f"   ì‹œì‘ ìŠ¤í…: {start_step}")
        print(f"   ëª©í‘œ ìŠ¤í…: {total_steps}")
        print(f"   ë‚¨ì€ ìŠ¤í…: {remaining_steps}")
        print(f"   ì§„í–‰ë¥ : {start_step/total_steps*100:.1f}%")

        # í•™ìŠµ ì‹œì‘
        steps, train_losses, val_losses = trainer.train(
            train_steps=total_steps, save_dir=args.save_dir
        )
    else:
        print(
            f"\nâœ… í•™ìŠµì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! (í˜„ì¬ ìŠ¤í…: {start_step}, ëª©í‘œ: {total_steps})"
        )
        return

    print(f"\ní•™ìŠµ ì™„ë£Œ! ê²°ê³¼ëŠ” {args.save_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def create_sample_config(config_name):
    """ìƒ˜í”Œ config íŒŒì¼ ìƒì„±"""
    sample_config = {
        "model": {
            "N": 6,
            "d_model": 512,
            "d_ff": 2048,
            "h": 8,
            "d_k": 64,
            "d_v": 64,
            "P_drop": 0.1,
            "max_seq_length": 128,
        },
        "training": {
            "train_steps": 10000,
            "batch_tokens": 25000,
            "learning_rate": 1e-4,
            "warmup_steps": 4000,
            "label_smoothing": 0.1,
            "grad_clip": 1.0,
            "eval_every": 500,
            "save_every": 1000,
        },
        "data": {"vocab_size": 5000, "max_length": 128, "data_multiplier": 10},
        "description": f"Custom config: {config_name}",
    }

    config_path = f"configs/{config_name}.json"
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(sample_config, f, indent=2, ensure_ascii=False)

    print(f"ìƒ˜í”Œ config íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {config_path}")
    print("íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ ì›í•˜ëŠ” ì„¤ì •ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()
